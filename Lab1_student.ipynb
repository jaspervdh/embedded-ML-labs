{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10",
      "metadata": {
        "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10"
      },
      "source": [
        "# Lab 1: pruning AI models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9",
      "metadata": {
        "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9"
      },
      "source": [
        "## Intro\n",
        "\n",
        "In this lab you will learn to **optimize** and **prune AI models** using the **LiteRT** library (previously called Tensorflow Lite \\[For Microcontrollers]). <br />\n",
        "\n",
        "\n",
        "To be able to run the necessary scripts throughout this lab, you will need access to a GPU. You can either **make use of your own GPU** (through a Linux or Windows WSL system, with a GPU-enabled tensorflow installed (version 2.18.0)) **or use Google Colab**. <br /><br />\n",
        "\n",
        "**To run this notebook in colab, you will need use the zip from Lab 0 and place this file in the extracted folder.** <br /><br />Instructions lab 0: download the lab folder on Ufora, **unzip it and put it on your Google Drive** (this folder will only be a few MBs in size). You can **drag and drop** the unzipped folder in your Google Drive.<br />\n",
        "\n",
        "\n",
        "Next, **double click on the provided .ipynb file** for each lab which will open Google Colab. <br />From there, fill in the necessary variables (such as the path to your Google Drive) and you will be able to **run and program the necessary code. Be sure te select a GPU under Runtime > Change runtime type.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28631c52-c731-43f8-8040-2e2287b40b15",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "28631c52-c731-43f8-8040-2e2287b40b15",
        "outputId": "7c93e6e6-e02b-4a46-f5b0-277451226f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (0.1.9)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n",
            "Requirement already satisfied: tf_keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf_keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf_keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install --user --upgrade tensorflow-model-optimization\n",
        "%pip install tf_keras\n",
        "\n",
        "# Click Runtime > Restart session\n",
        "# This ensures the above installed libraries are correctly imported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mRbq3kF3pDnj",
      "metadata": {
        "id": "mRbq3kF3pDnj"
      },
      "outputs": [],
      "source": [
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523f151c-faff-48cf-a37a-e6dc515112d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523f151c-faff-48cf-a37a-e6dc515112d9",
        "outputId": "9e095268-8bfd-4902-d0cf-13e881816871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Run this code to connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e534fb8-5544-4d33-b323-6ff13a02c709",
      "metadata": {
        "id": "3e534fb8-5544-4d33-b323-6ff13a02c709"
      },
      "outputs": [],
      "source": [
        "# Change to your project directory\n",
        "path_to_lab = \"drive/MyDrive/embedded-ML/labs\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22770a89-f8d3-4175-966b-b594758058b4",
      "metadata": {
        "id": "22770a89-f8d3-4175-966b-b594758058b4"
      },
      "source": [
        "## Functions\n",
        "Below you can find **functions** which can be used to complete the lab. <br />\n",
        "_Note: when running the below code for the first time on Google Colab, you will get a warning that you need to restart your runtime session. This is expected because the kernel needs to use the expected tensorflow version._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47935bbb-4e4b-4863-87c7-b18e977abfee",
      "metadata": {
        "id": "47935bbb-4e4b-4863-87c7-b18e977abfee",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras as keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def mnist_model(train=False):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=(6, 6), activation=tf.nn.relu, name=\"conv1\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, name=\"conv2\"),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(16, activation=tf.nn.relu, name=\"dense1\"),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"dense2\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    if train:\n",
        "        model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels))\n",
        "    else:\n",
        "        # model = tf.keras.models.load_model(\"Models/mnist.keras\")\n",
        "        model = tf.keras.models.load_model(f\"{path_to_lab}/Models/mnist\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39Pni1xAdeJ",
      "metadata": {
        "id": "b39Pni1xAdeJ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# shutil.rmtree(f'{path_to_lab}/Models/mnist_pruned_50pct')\n",
        "# shutil.rmtree(f'{path_to_lab}/Models/mnist_pruned_95pct')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43326446-1647-4367-a28e-7abc03e57c08",
      "metadata": {
        "id": "43326446-1647-4367-a28e-7abc03e57c08"
      },
      "source": [
        "## Part 1: convert models using LiteRT\n",
        "\n",
        "1) Load the mnist dataset and pre-trained model. For this exercise we will use a pre-trained model working on the mnist dataset for digit recognition.\n",
        "2) Evaluate the model. To obtain a baseline performance, evaluate the model without any LiteRT optimizations applied.\n",
        "3) Convert the model to the LiteRT format and evaluate whether this has an impact on performance or not.\n",
        "\n",
        "**Q1: Did you need to change anything to your (test)dataset?**\n",
        "\n",
        "**Q2: Do you see any difference in accuracy compared to the baseline?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907bb3c4-2706-4a9f-bc9f-1af6f7096a28",
      "metadata": {
        "id": "907bb3c4-2706-4a9f-bc9f-1af6f7096a28",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Load pre-trained model\n",
        "model = mnist_model(train=False)\n",
        "model.save(\"Models/mnist\")\n",
        "\n",
        "# Verify performance by inserting your code below\n",
        "# ---- see lab 0\n",
        "\n",
        "# Perform lite model conversion\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(f\"{path_to_lab}/Models/mnist\") # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(f'{path_to_lab}/Models/mnist.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScJ_lszij93P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScJ_lszij93P",
        "outputId": "47014e6f-cd2d-44b7-b670-5d8b75c5a183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1 28 28]\n",
            "(1, 28, 28)\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Verify performance of lite model\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=f\"{path_to_lab}/Models/mnist.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "test_image = test_images[0].astype(np.float32)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "print(input_shape)\n",
        "print(test_image.shape)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "interpreter.invoke()\n",
        "\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "predicted_label = np.argmax(output_data)\n",
        "print(test_labels[0] == predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jvn2pFcolW8o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvn2pFcolW8o",
        "outputId": "d8627a33-94ab-486a-d43a-68825b677776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite Model Accuracy: 0.9912\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "for i in range(len(test_images)):\n",
        "\n",
        "    # change type of array elements form UINT to float32\n",
        "    test_image = test_images[i].astype(np.float32)\n",
        "    # change shape of test img to be batch of lenght 1\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    # input test_image\n",
        "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "\n",
        "    # run model\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # get result\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    if np.argmax(output_data) == test_labels[i]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(test_images)\n",
        "print(f\"TFLite Model Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a58e0f7-76a2-4c82-af90-b609973cb49c",
      "metadata": {
        "id": "8a58e0f7-76a2-4c82-af90-b609973cb49c"
      },
      "source": [
        "## Part 2: prune optimized model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W_ChmFfBueJI",
      "metadata": {
        "id": "W_ChmFfBueJI"
      },
      "source": [
        "4) Prune all layers of the model, once at 50\\%, once at 95\\%. You can find\n",
        "\n",
        "---\n",
        "\n",
        "information on how to prune keras models [here](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JDkd2AHfufUi",
      "metadata": {
        "id": "JDkd2AHfufUi"
      },
      "outputs": [],
      "source": [
        "def prune_model(model, sparsity, epochs=15):\n",
        "    model_copy = tf.keras.models.clone_model(model)\n",
        "    model_copy.set_weights(model.get_weights())\n",
        "\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(\n",
        "            target_sparsity=sparsity,\n",
        "            begin_step=0,\n",
        "            end_step=int(train_images.shape[0] / 64 * epochs)  # batches of 64\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Apply pruning to the copy\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "    model_for_pruning = prune_low_magnitude(model_copy, **pruning_params)\n",
        "\n",
        "    model_for_pruning.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model_for_pruning\n",
        "\n",
        "def train_pruned_model(model, sparsity_level, epochs=15):\n",
        "\n",
        "    callbacks = [\n",
        "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "        tfmot.sparsity.keras.PruningSummaries(log_dir=f'logs/pruning_{int(sparsity_level*100)}pct'),\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_images, train_labels,\n",
        "        batch_size=64,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_images, test_labels),\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # final sparsity mask\n",
        "    model = tfmot.sparsity.keras.strip_pruning(model)\n",
        "\n",
        "    # save pruned model\n",
        "    model.save(f'{path_to_lab}/Models/mnist_pruned_{int(sparsity_level*100)}pct')\n",
        "\n",
        "    # convert to TFLite\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(f'{path_to_lab}/Models/mnist_pruned_{int(sparsity_level*100)}pct.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S9PvChj1w8Hu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9PvChj1w8Hu",
        "outputId": "db705859-9831-4828-edc0-47d96ee8ac22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  1/938 [..............................] - ETA: 46:38 - loss: 8.1515e-04 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0054s vs `on_train_batch_end` time: 0.0061s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 12s 10ms/step - loss: 2.3207 - accuracy: 0.1917 - val_loss: 2.3846 - val_accuracy: 0.0974\n",
            "Epoch 2/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 2.3395 - accuracy: 0.0975 - val_loss: 2.3136 - val_accuracy: 0.0974\n",
            "Epoch 3/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 2.2620 - accuracy: 0.1435 - val_loss: 2.1170 - val_accuracy: 0.2236\n",
            "Epoch 4/20\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 2.0902 - accuracy: 0.2334 - val_loss: 1.9202 - val_accuracy: 0.3454\n",
            "Epoch 5/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 1.9610 - accuracy: 0.3113 - val_loss: 1.7985 - val_accuracy: 0.3857\n",
            "Epoch 6/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.8755 - accuracy: 0.3421 - val_loss: 1.7005 - val_accuracy: 0.4476\n",
            "Epoch 7/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.7999 - accuracy: 0.3744 - val_loss: 1.5009 - val_accuracy: 0.5099\n",
            "Epoch 8/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 1.6612 - accuracy: 0.4270 - val_loss: 1.4015 - val_accuracy: 0.5237\n",
            "Epoch 9/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 1.6047 - accuracy: 0.4424 - val_loss: 1.3238 - val_accuracy: 0.5405\n",
            "Epoch 10/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 1.5521 - accuracy: 0.4586 - val_loss: 1.2887 - val_accuracy: 0.5434\n",
            "Epoch 11/20\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 1.5109 - accuracy: 0.4703 - val_loss: 1.2588 - val_accuracy: 0.5470\n",
            "Epoch 12/20\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 1.4992 - accuracy: 0.4731 - val_loss: 1.2454 - val_accuracy: 0.5474\n",
            "Epoch 13/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 1.4776 - accuracy: 0.4773 - val_loss: 1.2288 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 1.4663 - accuracy: 0.4810 - val_loss: 1.2164 - val_accuracy: 0.5498\n",
            "Epoch 15/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 1.4479 - accuracy: 0.4833 - val_loss: 1.2058 - val_accuracy: 0.5520\n",
            "Epoch 16/20\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 1.4318 - accuracy: 0.4883 - val_loss: 1.1961 - val_accuracy: 0.5527\n",
            "Epoch 17/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.4310 - accuracy: 0.4879 - val_loss: 1.1938 - val_accuracy: 0.5522\n",
            "Epoch 18/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.4233 - accuracy: 0.4906 - val_loss: 1.1902 - val_accuracy: 0.5539\n",
            "Epoch 19/20\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 1.4191 - accuracy: 0.4895 - val_loss: 1.1889 - val_accuracy: 0.5548\n",
            "Epoch 20/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 1.4177 - accuracy: 0.4895 - val_loss: 1.1846 - val_accuracy: 0.5549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.engine.sequential.Sequential at 0x7dd0e57204d0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model_50 = prune_model(model, 0.5)\n",
        "# train_pruned_model(model_50, 0.5)\n",
        "\n",
        "model_95 = prune_model(model, 0.95)\n",
        "train_pruned_model(model_95, 0.95, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ykBXwGAyxHse",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykBXwGAyxHse",
        "outputId": "f461dfec-ea5a-4b20-f129-351e720ce1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline test accuracy: 0.9912\n",
            "50% Pruned test accuracy: 0.9921\n",
            "95% Pruned test accuracy: 0.5549\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f'Baseline test accuracy: {test_acc:.4f}')\n",
        "\n",
        "test_loss, test_acc = model_50.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f'50% Pruned test accuracy: {test_acc:.4f}')\n",
        "\n",
        "test_loss, test_acc = model_95.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f'95% Pruned test accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o-jvUjY-1B87",
      "metadata": {
        "id": "o-jvUjY-1B87"
      },
      "source": [
        "5) Ensure all layers have the correct sparsity\n",
        "\n",
        "**Q3: what difference do you see in accuracy?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4S5mufL71Cql",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S5mufL71Cql",
        "outputId": "24227f23-e69b-4bbb-922d-274490318765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   layer_index                  layer_name      weight_name  sparsity\n",
            "0            1   prune_low_magnitude_conv1   conv1/kernel:0       0.5\n",
            "1            4   prune_low_magnitude_conv2   conv2/kernel:0       0.5\n",
            "2            7  prune_low_magnitude_dense1  dense1/kernel:0       0.5\n",
            "3            8  prune_low_magnitude_dense2  dense2/kernel:0       0.5\n",
            "   layer_index                  layer_name      weight_name  sparsity\n",
            "0            1   prune_low_magnitude_conv1   conv1/kernel:0  0.950087\n",
            "1            4   prune_low_magnitude_conv2   conv2/kernel:0  0.949978\n",
            "2            7  prune_low_magnitude_dense1  dense1/kernel:0  0.949990\n",
            "3            8  prune_low_magnitude_dense2  dense2/kernel:0  0.950000\n"
          ]
        }
      ],
      "source": [
        "def calculate_sparsity(weights):\n",
        "    \"\"\"Calculate the sparsity (percentage of zeros) in the weights tensor.\"\"\"\n",
        "    total_params = tf.size(weights).numpy()\n",
        "    zero_params = tf.math.count_nonzero(tf.abs(weights) < 1e-10).numpy()\n",
        "    return (zero_params / total_params)\n",
        "\n",
        "def verify_layer_sparsity(model):\n",
        "    results = []\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        layer_name = layer.name\n",
        "\n",
        "        # skip layers without weights or non-prunable layers\n",
        "        if not layer.weights or isinstance(layer, (tf.keras.layers.Dropout,\n",
        "                                                tf.keras.layers.MaxPooling2D,\n",
        "                                                tf.keras.layers.Reshape)):\n",
        "            continue\n",
        "\n",
        "        # process each weight in the layer\n",
        "        for j, weight in enumerate(layer.weights):\n",
        "            weight_name = weight.name\n",
        "\n",
        "            if 'kernel' in weight_name or 'weight' in weight_name:\n",
        "                sparsity = calculate_sparsity(weight)\n",
        "\n",
        "                results.append({\n",
        "                    'layer_index': i,\n",
        "                    'layer_name': layer_name,\n",
        "                    'weight_name': weight_name,\n",
        "                    'sparsity': sparsity,\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(verify_layer_sparsity(model_50))\n",
        "print(verify_layer_sparsity(model_95))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZtZpayYLDBXS",
      "metadata": {
        "id": "ZtZpayYLDBXS"
      },
      "source": [
        "6) Iterate pruning individually on the following layers: _[\"conv1\",\"conv2\",\"dense1\",\"dense2\"]_ and using the following sparsity levels: _[0.5, 0.85, 0.95, 0.99]_.\n",
        "   \n",
        "   _Hint: You can find more information about layer-based pruning [here](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yuL6Hz_REYRt",
      "metadata": {
        "id": "yuL6Hz_REYRt"
      },
      "outputs": [],
      "source": [
        "def prune_model_layer(model, layer_name, sparsity, epochs=15):\n",
        "    \"\"\"Prune only a specific layer in the model.\"\"\"\n",
        "    # Create a clean copy of the model\n",
        "    model_copy = tf.keras.models.clone_model(model)\n",
        "    model_copy.set_weights(model.get_weights())\n",
        "\n",
        "    # Configure layer-wise pruning by setting up pruning_configs dictionary\n",
        "    pruning_configs = []\n",
        "\n",
        "    # Define the pruning schedule\n",
        "    pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(\n",
        "        target_sparsity=sparsity,\n",
        "        begin_step=0,\n",
        "        end_step=int(train_images.shape[0] / 64 * epochs)  # batches of 64\n",
        "    )\n",
        "\n",
        "    # Find the target layer by name and configure just that layer for pruning\n",
        "    for layer in model_copy.layers:\n",
        "        if layer.name == layer_name:\n",
        "            pruning_configs.append({\n",
        "                'layer': layer,\n",
        "                'pruning_schedule': pruning_schedule\n",
        "            })\n",
        "            break\n",
        "\n",
        "    # Apply the layer-specific pruning configuration\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "    model_for_pruning = prune_low_magnitude(\n",
        "        model_copy,\n",
        "        pruning_configs=pruning_configs\n",
        "    )\n",
        "\n",
        "    # Compile the pruned model\n",
        "    model_for_pruning.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model_for_pruning\n",
        "\n",
        "def train_pruned_layer_model(model, layer_name, sparsity_level, epochs=15):\n",
        "    \"\"\"Train a model with a specific layer pruned.\"\"\"\n",
        "    callbacks = [\n",
        "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "        tfmot.sparsity.keras.PruningSummaries(log_dir=f'logs/pruning_{layer_name}_{int(sparsity_level*100)}pct'),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_images, train_labels,\n",
        "        batch_size=64,\n",
        "        epochs=epochs,\n",
        "        validation_data=(test_images, test_labels),\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Strip final sparsity mask\n",
        "    stripped_model = tfmot.sparsity.keras.strip_pruning(model)\n",
        "\n",
        "    # Recompile the stripped model\n",
        "    stripped_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Save pruned model\n",
        "    model_path = f'{path_to_lab}/Models/mnist_pruned_{layer_name}_{int(sparsity_level*100)}pct'\n",
        "    stripped_model.save(model_path)\n",
        "\n",
        "    # Convert to TFLite\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(f'{model_path}.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    return stripped_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ajASQEBODA1L",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajASQEBODA1L",
        "outputId": "58294a9a-257a-4821-c465-9501a36d036b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline test accuracy: 0.9912\n",
            "\n",
            "Pruning layer 'conv1' with 50% sparsity\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  1/938 [..............................] - ETA: 1:00:23 - loss: 0.2726 - accuracy: 0.9375"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0070s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2550 - accuracy: 0.9178 - val_loss: 0.0666 - val_accuracy: 0.9821\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1055 - accuracy: 0.9727 - val_loss: 0.0462 - val_accuracy: 0.9882\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0796 - accuracy: 0.9787 - val_loss: 0.0422 - val_accuracy: 0.9893\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0699 - accuracy: 0.9813 - val_loss: 0.0348 - val_accuracy: 0.9909\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0614 - accuracy: 0.9830 - val_loss: 0.0338 - val_accuracy: 0.9908\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0544 - accuracy: 0.9840 - val_loss: 0.0324 - val_accuracy: 0.9908\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.0332 - val_accuracy: 0.9909\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0456 - accuracy: 0.9866 - val_loss: 0.0283 - val_accuracy: 0.9923\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0427 - accuracy: 0.9876 - val_loss: 0.0268 - val_accuracy: 0.9918\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0422 - accuracy: 0.9877 - val_loss: 0.0263 - val_accuracy: 0.9924\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 0.0278 - val_accuracy: 0.9918\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0408 - accuracy: 0.9881 - val_loss: 0.0270 - val_accuracy: 0.9921\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0384 - accuracy: 0.9887 - val_loss: 0.0270 - val_accuracy: 0.9922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv1' @ 50% pruned test accuracy: 0.0980\n",
            "\n",
            "Pruning layer 'conv1' with 85% sparsity\n",
            "Epoch 1/15\n",
            "938/938 [==============================] - 11s 9ms/step - loss: 0.2650 - accuracy: 0.9136 - val_loss: 0.0694 - val_accuracy: 0.9805\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.1180 - accuracy: 0.9669 - val_loss: 0.0529 - val_accuracy: 0.9849\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0855 - accuracy: 0.9766 - val_loss: 0.0387 - val_accuracy: 0.9889\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0701 - accuracy: 0.9805 - val_loss: 0.0390 - val_accuracy: 0.9895\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0611 - accuracy: 0.9825 - val_loss: 0.0327 - val_accuracy: 0.9904\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0580 - accuracy: 0.9836 - val_loss: 0.0334 - val_accuracy: 0.9907\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0539 - accuracy: 0.9844 - val_loss: 0.0323 - val_accuracy: 0.9910\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0496 - accuracy: 0.9856 - val_loss: 0.0348 - val_accuracy: 0.9907\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0474 - accuracy: 0.9863 - val_loss: 0.0309 - val_accuracy: 0.9916\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0455 - accuracy: 0.9865 - val_loss: 0.0307 - val_accuracy: 0.9917\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0429 - accuracy: 0.9873 - val_loss: 0.0303 - val_accuracy: 0.9921\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.0340 - val_accuracy: 0.9917\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0394 - accuracy: 0.9885 - val_loss: 0.0308 - val_accuracy: 0.9914\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 0.0280 - val_accuracy: 0.9929\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0395 - accuracy: 0.9885 - val_loss: 0.0263 - val_accuracy: 0.9926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv1' @ 85% pruned test accuracy: 0.0981\n",
            "\n",
            "Pruning layer 'conv1' with 95% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 59:32 - loss: 0.0043 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0059s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 7ms/step - loss: 0.2671 - accuracy: 0.9153 - val_loss: 0.0724 - val_accuracy: 0.9805\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.1192 - accuracy: 0.9702 - val_loss: 0.0517 - val_accuracy: 0.9864\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0902 - accuracy: 0.9772 - val_loss: 0.0446 - val_accuracy: 0.9887\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0752 - accuracy: 0.9801 - val_loss: 0.0390 - val_accuracy: 0.9895\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0657 - accuracy: 0.9822 - val_loss: 0.0347 - val_accuracy: 0.9900\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0637 - accuracy: 0.9830 - val_loss: 0.0329 - val_accuracy: 0.9914\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0588 - accuracy: 0.9837 - val_loss: 0.0327 - val_accuracy: 0.9909\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0546 - accuracy: 0.9847 - val_loss: 0.0318 - val_accuracy: 0.9908\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0501 - accuracy: 0.9854 - val_loss: 0.0321 - val_accuracy: 0.9909\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.0292 - val_accuracy: 0.9915\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0455 - accuracy: 0.9867 - val_loss: 0.0296 - val_accuracy: 0.9912\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0437 - accuracy: 0.9873 - val_loss: 0.0308 - val_accuracy: 0.9915\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0430 - accuracy: 0.9873 - val_loss: 0.0281 - val_accuracy: 0.9916\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0431 - accuracy: 0.9874 - val_loss: 0.0279 - val_accuracy: 0.9913\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0408 - accuracy: 0.9878 - val_loss: 0.0286 - val_accuracy: 0.9920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv1' @ 95% pruned test accuracy: 0.0979\n",
            "\n",
            "Pruning layer 'conv1' with 99% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 43:43 - loss: 0.0072 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2513 - accuracy: 0.9191 - val_loss: 0.0642 - val_accuracy: 0.9824\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1049 - accuracy: 0.9719 - val_loss: 0.0474 - val_accuracy: 0.9873\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0820 - accuracy: 0.9781 - val_loss: 0.0375 - val_accuracy: 0.9887\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0676 - accuracy: 0.9814 - val_loss: 0.0337 - val_accuracy: 0.9904\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0590 - accuracy: 0.9835 - val_loss: 0.0360 - val_accuracy: 0.9907\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0552 - accuracy: 0.9845 - val_loss: 0.0320 - val_accuracy: 0.9906\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0517 - accuracy: 0.9850 - val_loss: 0.0306 - val_accuracy: 0.9920\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0475 - accuracy: 0.9862 - val_loss: 0.0298 - val_accuracy: 0.9917\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.0273 - val_accuracy: 0.9906\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0427 - accuracy: 0.9877 - val_loss: 0.0313 - val_accuracy: 0.9902\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 0.0278 - val_accuracy: 0.9912\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.0309 - val_accuracy: 0.9909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv1' @ 99% pruned test accuracy: 0.0981\n",
            "\n",
            "Pruning layer 'conv2' with 50% sparsity\n",
            "Epoch 1/15\n",
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2788 - accuracy: 0.9199 - val_loss: 0.0867 - val_accuracy: 0.9836\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.1118 - accuracy: 0.9724 - val_loss: 0.0452 - val_accuracy: 0.9886\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0819 - accuracy: 0.9781 - val_loss: 0.0413 - val_accuracy: 0.9874\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0714 - accuracy: 0.9802 - val_loss: 0.0381 - val_accuracy: 0.9894\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0621 - accuracy: 0.9830 - val_loss: 0.0357 - val_accuracy: 0.9898\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0568 - accuracy: 0.9846 - val_loss: 0.0324 - val_accuracy: 0.9904\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0524 - accuracy: 0.9855 - val_loss: 0.0288 - val_accuracy: 0.9908\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0482 - accuracy: 0.9862 - val_loss: 0.0298 - val_accuracy: 0.9917\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0468 - accuracy: 0.9866 - val_loss: 0.0302 - val_accuracy: 0.9916\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0427 - accuracy: 0.9871 - val_loss: 0.0319 - val_accuracy: 0.9918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv2' @ 50% pruned test accuracy: 0.0984\n",
            "\n",
            "Pruning layer 'conv2' with 85% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 43:34 - loss: 0.0796 - accuracy: 0.9844"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 10s 8ms/step - loss: 0.2965 - accuracy: 0.9039 - val_loss: 0.0669 - val_accuracy: 0.9828\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1092 - accuracy: 0.9719 - val_loss: 0.0509 - val_accuracy: 0.9861\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0840 - accuracy: 0.9784 - val_loss: 0.0419 - val_accuracy: 0.9883\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0715 - accuracy: 0.9805 - val_loss: 0.0376 - val_accuracy: 0.9889\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0637 - accuracy: 0.9825 - val_loss: 0.0391 - val_accuracy: 0.9899\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0346 - val_accuracy: 0.9897\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0560 - accuracy: 0.9846 - val_loss: 0.0345 - val_accuracy: 0.9903\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0505 - accuracy: 0.9857 - val_loss: 0.0286 - val_accuracy: 0.9910\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0467 - accuracy: 0.9864 - val_loss: 0.0315 - val_accuracy: 0.9913\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0442 - accuracy: 0.9871 - val_loss: 0.0309 - val_accuracy: 0.9905\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0464 - accuracy: 0.9869 - val_loss: 0.0283 - val_accuracy: 0.9916\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0433 - accuracy: 0.9873 - val_loss: 0.0293 - val_accuracy: 0.9917\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0400 - accuracy: 0.9879 - val_loss: 0.0285 - val_accuracy: 0.9922\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0394 - accuracy: 0.9884 - val_loss: 0.0262 - val_accuracy: 0.9923\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0363 - accuracy: 0.9893 - val_loss: 0.0307 - val_accuracy: 0.9910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv2' @ 85% pruned test accuracy: 0.0984\n",
            "\n",
            "Pruning layer 'conv2' with 95% sparsity\n",
            "Epoch 1/15\n",
            "  6/938 [..............................] - ETA: 10s - loss: 0.0705 - accuracy: 0.9844    "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0121s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 12s 8ms/step - loss: 0.2670 - accuracy: 0.9226 - val_loss: 0.0735 - val_accuracy: 0.9854\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.1048 - accuracy: 0.9742 - val_loss: 0.0402 - val_accuracy: 0.9885\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0735 - accuracy: 0.9805 - val_loss: 0.0352 - val_accuracy: 0.9891\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0623 - accuracy: 0.9823 - val_loss: 0.0316 - val_accuracy: 0.9907\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0568 - accuracy: 0.9840 - val_loss: 0.0329 - val_accuracy: 0.9906\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0506 - accuracy: 0.9855 - val_loss: 0.0256 - val_accuracy: 0.9920\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0270 - val_accuracy: 0.9921\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0440 - accuracy: 0.9876 - val_loss: 0.0283 - val_accuracy: 0.9919\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.0277 - val_accuracy: 0.9918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv2' @ 95% pruned test accuracy: 0.0982\n",
            "\n",
            "Pruning layer 'conv2' with 99% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 45:24 - loss: 0.0022 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0064s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 9ms/step - loss: 0.2914 - accuracy: 0.9009 - val_loss: 0.0705 - val_accuracy: 0.9806\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1041 - accuracy: 0.9714 - val_loss: 0.0481 - val_accuracy: 0.9867\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0814 - accuracy: 0.9779 - val_loss: 0.0362 - val_accuracy: 0.9887\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0647 - accuracy: 0.9809 - val_loss: 0.0384 - val_accuracy: 0.9894\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0329 - val_accuracy: 0.9908\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0555 - accuracy: 0.9843 - val_loss: 0.0313 - val_accuracy: 0.9911\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0524 - accuracy: 0.9850 - val_loss: 0.0321 - val_accuracy: 0.9911\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0497 - accuracy: 0.9852 - val_loss: 0.0309 - val_accuracy: 0.9912\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.0309 - val_accuracy: 0.9919\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0449 - accuracy: 0.9868 - val_loss: 0.0293 - val_accuracy: 0.9909\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0418 - accuracy: 0.9877 - val_loss: 0.0294 - val_accuracy: 0.9914\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0434 - accuracy: 0.9878 - val_loss: 0.0303 - val_accuracy: 0.9910\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0407 - accuracy: 0.9880 - val_loss: 0.0313 - val_accuracy: 0.9915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'conv2' @ 99% pruned test accuracy: 0.0987\n",
            "\n",
            "Pruning layer 'dense1' with 50% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 43:18 - loss: 0.1929 - accuracy: 0.9531"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2302 - accuracy: 0.9313 - val_loss: 0.0602 - val_accuracy: 0.9860\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1027 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 0.9880\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0771 - accuracy: 0.9805 - val_loss: 0.0370 - val_accuracy: 0.9904\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0698 - accuracy: 0.9816 - val_loss: 0.0353 - val_accuracy: 0.9902\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0598 - accuracy: 0.9832 - val_loss: 0.0362 - val_accuracy: 0.9904\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0535 - accuracy: 0.9847 - val_loss: 0.0314 - val_accuracy: 0.9911\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0529 - accuracy: 0.9858 - val_loss: 0.0310 - val_accuracy: 0.9916\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0484 - accuracy: 0.9862 - val_loss: 0.0315 - val_accuracy: 0.9910\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0458 - accuracy: 0.9867 - val_loss: 0.0300 - val_accuracy: 0.9916\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0434 - accuracy: 0.9871 - val_loss: 0.0307 - val_accuracy: 0.9912\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.0327 - val_accuracy: 0.9905\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.0344 - val_accuracy: 0.9906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense1' @ 50% pruned test accuracy: 0.0984\n",
            "\n",
            "Pruning layer 'dense1' with 85% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 43:39 - loss: 4.5116e-04 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0053s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2816 - accuracy: 0.9070 - val_loss: 0.0626 - val_accuracy: 0.9825\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.1020 - accuracy: 0.9725 - val_loss: 0.0436 - val_accuracy: 0.9862\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0804 - accuracy: 0.9784 - val_loss: 0.0360 - val_accuracy: 0.9894\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0648 - accuracy: 0.9827 - val_loss: 0.0350 - val_accuracy: 0.9893\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0579 - accuracy: 0.9838 - val_loss: 0.0332 - val_accuracy: 0.9905\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0513 - accuracy: 0.9852 - val_loss: 0.0305 - val_accuracy: 0.9909\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0496 - accuracy: 0.9856 - val_loss: 0.0362 - val_accuracy: 0.9890\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0326 - val_accuracy: 0.9911\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 0.0265 - val_accuracy: 0.9921\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0408 - accuracy: 0.9879 - val_loss: 0.0308 - val_accuracy: 0.9914\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0432 - accuracy: 0.9871 - val_loss: 0.0285 - val_accuracy: 0.9907\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0439 - accuracy: 0.9871 - val_loss: 0.0293 - val_accuracy: 0.9917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense1' @ 85% pruned test accuracy: 0.0985\n",
            "\n",
            "Pruning layer 'dense1' with 95% sparsity\n",
            "Epoch 1/15\n",
            "938/938 [==============================] - 11s 8ms/step - loss: 0.2458 - accuracy: 0.9208 - val_loss: 0.0629 - val_accuracy: 0.9820\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1108 - accuracy: 0.9710 - val_loss: 0.0439 - val_accuracy: 0.9875\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0854 - accuracy: 0.9774 - val_loss: 0.0361 - val_accuracy: 0.9898\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0720 - accuracy: 0.9797 - val_loss: 0.0396 - val_accuracy: 0.9882\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0658 - accuracy: 0.9814 - val_loss: 0.0368 - val_accuracy: 0.9894\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0623 - accuracy: 0.9830 - val_loss: 0.0318 - val_accuracy: 0.9916\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0524 - accuracy: 0.9848 - val_loss: 0.0305 - val_accuracy: 0.9916\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0517 - accuracy: 0.9854 - val_loss: 0.0278 - val_accuracy: 0.9914\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0466 - accuracy: 0.9867 - val_loss: 0.0298 - val_accuracy: 0.9919\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0468 - accuracy: 0.9865 - val_loss: 0.0300 - val_accuracy: 0.9912\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0456 - accuracy: 0.9873 - val_loss: 0.0287 - val_accuracy: 0.9915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense1' @ 95% pruned test accuracy: 0.0982\n",
            "\n",
            "Pruning layer 'dense1' with 99% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 43:44 - loss: 0.0057 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 9ms/step - loss: 0.2581 - accuracy: 0.9216 - val_loss: 0.0645 - val_accuracy: 0.9848\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1105 - accuracy: 0.9737 - val_loss: 0.0500 - val_accuracy: 0.9876\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0892 - accuracy: 0.9773 - val_loss: 0.0451 - val_accuracy: 0.9883\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0723 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9884\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0670 - accuracy: 0.9817 - val_loss: 0.0352 - val_accuracy: 0.9903\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0362 - val_accuracy: 0.9907\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0552 - accuracy: 0.9848 - val_loss: 0.0339 - val_accuracy: 0.9904\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0538 - accuracy: 0.9848 - val_loss: 0.0332 - val_accuracy: 0.9899\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0533 - accuracy: 0.9855 - val_loss: 0.0321 - val_accuracy: 0.9909\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0468 - accuracy: 0.9866 - val_loss: 0.0333 - val_accuracy: 0.9909\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0461 - accuracy: 0.9869 - val_loss: 0.0304 - val_accuracy: 0.9911\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.0425 - accuracy: 0.9877 - val_loss: 0.0302 - val_accuracy: 0.9916\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0401 - accuracy: 0.9883 - val_loss: 0.0308 - val_accuracy: 0.9909\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0424 - accuracy: 0.9878 - val_loss: 0.0324 - val_accuracy: 0.9901\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0399 - accuracy: 0.9883 - val_loss: 0.0354 - val_accuracy: 0.9903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense1' @ 99% pruned test accuracy: 0.0984\n",
            "\n",
            "Pruning layer 'dense2' with 50% sparsity\n",
            "Epoch 1/15\n",
            "938/938 [==============================] - 11s 9ms/step - loss: 0.2692 - accuracy: 0.9072 - val_loss: 0.0624 - val_accuracy: 0.9811\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1021 - accuracy: 0.9722 - val_loss: 0.0474 - val_accuracy: 0.9865\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0738 - accuracy: 0.9794 - val_loss: 0.0387 - val_accuracy: 0.9889\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0622 - accuracy: 0.9826 - val_loss: 0.0382 - val_accuracy: 0.9900\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0531 - accuracy: 0.9851 - val_loss: 0.0375 - val_accuracy: 0.9895\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0524 - accuracy: 0.9850 - val_loss: 0.0357 - val_accuracy: 0.9902\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0486 - accuracy: 0.9863 - val_loss: 0.0318 - val_accuracy: 0.9908\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0464 - accuracy: 0.9863 - val_loss: 0.0325 - val_accuracy: 0.9899\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 0.0324 - val_accuracy: 0.9905\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0405 - accuracy: 0.9886 - val_loss: 0.0330 - val_accuracy: 0.9911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense2' @ 50% pruned test accuracy: 0.0979\n",
            "\n",
            "Pruning layer 'dense2' with 85% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 44:41 - loss: 0.0029 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0056s vs `on_train_batch_end` time: 0.0068s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 11s 9ms/step - loss: 0.2793 - accuracy: 0.9103 - val_loss: 0.0802 - val_accuracy: 0.9816\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1083 - accuracy: 0.9722 - val_loss: 0.0479 - val_accuracy: 0.9870\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0834 - accuracy: 0.9780 - val_loss: 0.0430 - val_accuracy: 0.9876\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0707 - accuracy: 0.9809 - val_loss: 0.0392 - val_accuracy: 0.9881\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0627 - accuracy: 0.9831 - val_loss: 0.0333 - val_accuracy: 0.9898\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0343 - val_accuracy: 0.9895\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0519 - accuracy: 0.9848 - val_loss: 0.0321 - val_accuracy: 0.9900\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0504 - accuracy: 0.9855 - val_loss: 0.0310 - val_accuracy: 0.9908\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0459 - accuracy: 0.9868 - val_loss: 0.0297 - val_accuracy: 0.9917\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 0.0297 - val_accuracy: 0.9910\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0434 - accuracy: 0.9873 - val_loss: 0.0260 - val_accuracy: 0.9920\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0425 - accuracy: 0.9880 - val_loss: 0.0300 - val_accuracy: 0.9911\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0397 - accuracy: 0.9883 - val_loss: 0.0299 - val_accuracy: 0.9912\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0399 - accuracy: 0.9879 - val_loss: 0.0265 - val_accuracy: 0.9911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense2' @ 85% pruned test accuracy: 0.0980\n",
            "\n",
            "Pruning layer 'dense2' with 95% sparsity\n",
            "Epoch 1/15\n",
            "938/938 [==============================] - 10s 7ms/step - loss: 0.2577 - accuracy: 0.9118 - val_loss: 0.0613 - val_accuracy: 0.9843\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1001 - accuracy: 0.9735 - val_loss: 0.0462 - val_accuracy: 0.9877\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0826 - accuracy: 0.9780 - val_loss: 0.0402 - val_accuracy: 0.9899\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0682 - accuracy: 0.9810 - val_loss: 0.0380 - val_accuracy: 0.9892\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0656 - accuracy: 0.9816 - val_loss: 0.0328 - val_accuracy: 0.9904\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0570 - accuracy: 0.9846 - val_loss: 0.0345 - val_accuracy: 0.9907\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0548 - accuracy: 0.9845 - val_loss: 0.0336 - val_accuracy: 0.9912\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0485 - accuracy: 0.9857 - val_loss: 0.0313 - val_accuracy: 0.9908\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0457 - accuracy: 0.9866 - val_loss: 0.0314 - val_accuracy: 0.9918\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.0311 - val_accuracy: 0.9911\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0443 - accuracy: 0.9873 - val_loss: 0.0331 - val_accuracy: 0.9903\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0315 - val_accuracy: 0.9907\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0416 - accuracy: 0.9876 - val_loss: 0.0303 - val_accuracy: 0.9903\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0278 - val_accuracy: 0.9923\n",
            "Epoch 15/15\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 0.0275 - val_accuracy: 0.9913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 'dense2' @ 95% pruned test accuracy: 0.0981\n",
            "\n",
            "Pruning layer 'dense2' with 99% sparsity\n",
            "Epoch 1/15\n",
            "  1/938 [..............................] - ETA: 46:39 - loss: 0.0023 - accuracy: 1.0000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0053s vs `on_train_batch_end` time: 0.0060s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 10s 7ms/step - loss: 0.2600 - accuracy: 0.9210 - val_loss: 0.0666 - val_accuracy: 0.9852\n",
            "Epoch 2/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.1153 - accuracy: 0.9719 - val_loss: 0.0473 - val_accuracy: 0.9881\n",
            "Epoch 3/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0870 - accuracy: 0.9780 - val_loss: 0.0433 - val_accuracy: 0.9889\n",
            "Epoch 4/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0718 - accuracy: 0.9814 - val_loss: 0.0408 - val_accuracy: 0.9887\n",
            "Epoch 5/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0669 - accuracy: 0.9812 - val_loss: 0.0351 - val_accuracy: 0.9907\n",
            "Epoch 6/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0576 - accuracy: 0.9839 - val_loss: 0.0312 - val_accuracy: 0.9906\n",
            "Epoch 7/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0541 - accuracy: 0.9845 - val_loss: 0.0325 - val_accuracy: 0.9904\n",
            "Epoch 8/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0525 - accuracy: 0.9847 - val_loss: 0.0317 - val_accuracy: 0.9905\n",
            "Epoch 9/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 0.0280 - val_accuracy: 0.9909\n",
            "Epoch 10/15\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.0451 - accuracy: 0.9865 - val_loss: 0.0282 - val_accuracy: 0.9919\n",
            "Epoch 11/15\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 0.0279 - val_accuracy: 0.9924\n",
            "Epoch 12/15\n",
            "938/938 [==============================] - 8s 9ms/step - loss: 0.0422 - accuracy: 0.9875 - val_loss: 0.0271 - val_accuracy: 0.9915\n",
            "Epoch 13/15\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 0.0291 - val_accuracy: 0.9915\n",
            "Epoch 14/15\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.0407 - accuracy: 0.9883 - val_loss: 0.0315 - val_accuracy: 0.9912\n",
            "Epoch 15/15\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0386 - accuracy: 0.9882"
          ]
        }
      ],
      "source": [
        "layers = [\"conv1\",\"conv2\",\"dense1\",\"dense2\"]\n",
        "sparsity_levels = [0.5, 0.85, 0.95, 0.99]\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "# First evaluate the baseline model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f'Baseline test accuracy: {test_acc:.4f}')\n",
        "results['baseline'] = test_acc\n",
        "\n",
        "# Iterate through each layer and sparsity level\n",
        "for layer_name in layers:\n",
        "    for sparsity in sparsity_levels:\n",
        "        print(f\"\\nPruning layer '{layer_name}' with {sparsity*100:.0f}% sparsity\")\n",
        "\n",
        "        model_pruned = prune_model_layer(model, layer_name, sparsity)\n",
        "\n",
        "        # train\n",
        "        pruned_model = train_pruned_layer_model(model_pruned, layer_name, sparsity)\n",
        "\n",
        "        # evaluate\n",
        "        test_loss, test_acc = pruned_model.evaluate(test_images, test_labels, verbose=0)\n",
        "        print(f\"Layer '{layer_name}' @ {sparsity*100:.0f}% pruned test accuracy: {test_acc:.4f}\")\n",
        "        results[f\"{layer_name}_{int(sparsity*100)}pct\"] = test_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d88c2f-1c66-4ef6-aaae-60ff1e31f1cd",
      "metadata": {
        "id": "90d88c2f-1c66-4ef6-aaae-60ff1e31f1cd"
      },
      "source": [
        "\n",
        "   \n",
        "**Q4: Generate the following figure (see also in the Ufora question) using the above results:**\n",
        "\n",
        "![PruningFigure.png](PruningFigure.png)\n",
        "\n",
        "**Q5: How much would you prune the model for an embedded ML application?**  \n",
        "\n",
        "**Q6: Can you see the lite model decrease in size? By how much? Tip: check the zipped file size**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd14b1b9-26a2-446b-ba69-62f2de5932b4",
      "metadata": {
        "id": "cd14b1b9-26a2-446b-ba69-62f2de5932b4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}