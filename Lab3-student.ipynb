{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10",
   "metadata": {},
   "source": [
    "# Lab 3: Embedded AI on an Arduino "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In this lab you will use the to **optimization**, **pruning** and **quantization** techniques from the previous labs using the **LiteRT** library (previously called Tensorflow Lite \\[For Microcontrollers]). <br />\n",
    "Next, you will **convert this model into a binary format** which can be compiled on the **Arduino Nano 33 BLE board**, which is provided to you. Here, you will be asked to **calculate the inference time and memory consumption.** \n",
    "\n",
    "\n",
    "To be able to run the necessary scripts throughout this lab, you will need access to a GPU. You can either **make use of your own GPU** (through a Linux or Windows WSL system, with a GPU-enabled tensorflow installed (version 2.18.0)) **or use Google Colab**. <br />To run notebooks in colab, you will need to download the lab folder on Ufora, **unzip it and put it on your Google Drive** (this folder will only be a few MBs in size). You can **drag and drop** the unzipped folder in your Google Drive.<br /><br />\n",
    "\n",
    "\n",
    "Next, **double click on the provided .ipynb file** for each lab which will open Google Colab. <br />From there, fill in the necessary variables (such as the path to your Google Drive) and you will be able to **run and program the necessary code. Be sure te select a GPU under Runtime > Change runtime type.**\n",
    "\n",
    "The **Arduino IDE** can be downloaded [here](https://www.arduino.cc/en/software) for Linux, Windows or MacOS systems.<br /> Next, you will need to put the Arduino TensorFlowLite library in [Documents]/Arduino/libraries/:\n",
    "\n",
    "```\n",
    "cd ~/Arduino/libraries or cd ~/Documents/Arduino/libraries/ or My Documents\\Arduino\\Libraries\n",
    "git clone https://github.com/tensorflow/tflite-micro-arduino-examples Arduino_TensorFlowLite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27feb72a-c720-421f-8694-c4e5c0f6ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf_keras in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tf_keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.25.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade tensorflow-model-optimization\n",
    "%pip install tf_keras\n",
    "\n",
    "# Click Runtime > Restart session\n",
    "# This ensures the above installed libraries are correctly imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9080cf22-1eac-4028-babb-64f62d56b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to connect your Google Drive to Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7b8cb9-80a5-4bd1-a33b-54125d2f37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your project directory\n",
    "path_to_lab = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672bdb8-df1b-4d36-b3ba-a026d2433f7b",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below you can find **functions** which can be used to complete the lab. <br />\n",
    "_Note: when running the below code for the first time on Google Colab, you will get a warning that you need to restart your runtime session. This is expected because the kernel needs to use the expected tensorflow version._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cda5a51-d1c7-4761-8fe3-6404e12ebff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 17:30:53.229696: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 17:30:53.298401: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 17:30:53.375522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-09 17:30:53.452047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-09 17:30:53.452600: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-09 17:30:53.563259: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 17:30:54.929048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf_keras==2.16.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tf_keras==2.16.0) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (4.25.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras==2.16.0) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras==2.16.0) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "%pip install --user --upgrade tensorflow-model-optimization\n",
    "%pip install tf_keras==2.16.0\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def mnist_model(train=False):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(6, 6), activation=tf.nn.relu, name=\"conv1\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, name=\"conv2\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, name=\"dense1\"),\n",
    "    # tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"dense2\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if train:\n",
    "        model.fit(x=train_images, y= train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))\n",
    "    else:\n",
    "        # model = tf.keras.models.load_model(\"Models/mnist.keras\")\n",
    "        model = tf.keras.models.load_model(path_to_lab + \"Models/mnist\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43326446-1647-4367-a28e-7abc03e57c08",
   "metadata": {},
   "source": [
    "## Part 1: optimize models using LiteRT\n",
    "\n",
    "1) Start from the pruned (first three layers, 85%) + INT8 quantized model of the last lab and save the model.\n",
    "2) Re-evaluate the model without any LiteRT optimizations applied (baseline performance) and with pruning + quantization (accuracy performance after optimizations). This should be similar as previous lab (>90%). If not, contact the instructor during the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae8caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code lab1: helper function to verify performance of tflite model\n",
    "def verify_performance(model_path):\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path_to_lab + model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # test_image = test_images[0].astype(np.float32)\n",
    "    # test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    # interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    # interpreter.invoke()\n",
    "\n",
    "    # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # predicted_label = np.argmax(output_data)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        # change type of array elements form UINT to float32\n",
    "        test_image = (test_images[i] - 128).astype(np.int8)\n",
    "        # change shape of test img to be batch of lenght 1\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "        # input test_image\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "        # run model\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # get result\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        if np.argmax(output_data) == test_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(test_images)\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"TFLite Model ({model_name}) Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# helper functions to check diff in models\n",
    "def print_model_details(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of tensors: {len(interpreter.get_tensor_details())}\")\n",
    "    print(f\"Number of ops: {len(interpreter.get_signature_list())}\")\n",
    "\n",
    "    for tensor in interpreter.get_tensor_details()[0:3]:\n",
    "        print(f\"Tensor Name: {tensor['name']}, Shape: {tensor['shape']}, Type: {tensor['dtype']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def check_weight_types(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    weight_types = {tensor['dtype'] for tensor in tensor_details}\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Weight data types: {weight_types}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_model(model_path, output_zip_path=None):\n",
    "    \"\"\"\n",
    "    Zips a model file and saves it to the specified output path.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the model file to be zipped\n",
    "        output_zip_path (str, optional): Path for the output zip file. \n",
    "                        If None, uses model_path + '.zip'\n",
    "    \"\"\"\n",
    "    if output_zip_path is None:\n",
    "        output_zip_path = model_path + '.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_path, os.path.basename(model_path))\n",
    "    \n",
    "    print(f\"Model zipped to: {output_zip_path}\")\n",
    "    print(f\"Original size: {os.path.getsize(model_path)} bytes\")\n",
    "    print(f\"Zipped size: {os.path.getsize(output_zip_path)} bytes\")\n",
    "    return output_zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf0368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#17 is a dynamic-sized tensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (mnist_pruned85_quantint8.tflite) Accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Part 1: Start from the pruned + INT8 quantized model from the previous lab\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "verify_performance('Models/mnist_pruned85_quantint8.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb714cb-dbf7-45d3-a462-edf71f1de927",
   "metadata": {},
   "source": [
    "## Part 2: Covert the model in binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fc74b",
   "metadata": {},
   "source": [
    "3) Covert LiteRT model to the binary format ready to be compiled on your Arduino Nano 33 BLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e7e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "! xxd -i Models/mnist_pruned85_quantint8.tflite > arduino/mnist_pruned85_quantint8.cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab6ab3",
   "metadata": {},
   "source": [
    "**Q1: What command did you use and how does the resulting file content look like?**\n",
    "- Used command: `xxd -i Models/mnist_pruned85_quantint8.tflite > arduino/mnist_pruned85_quantint8.cc`\n",
    "- File contains a C array, named `Models_mnist_pruned85_quantint8_tflite[]`. I contains the binary data of the TfLite model represented in hex values. It also contains a variable `Models_mnist_pruned85_quantint8_tflite_len` with value `70688`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d561791",
   "metadata": {},
   "source": [
    "4) Print a C array of a few input examples (from the test dataset) to be included in a testsamples.h file during compile time so that the model can be tested on real examples at your Arduino Nano 33 BLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd782d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_array_to_carray(array, name):\n",
    "    c_array = f\"const signed char {name}[{array.size}] = {{\\n  \"\n",
    "    flat_list = array.flatten()\n",
    "    c_array += \", \".join(str(x) for x in flat_list)\n",
    "    c_array += \"\\n};\\n\"\n",
    "\n",
    "    return c_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e129a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "model_path = \"Models/mnist_pruned85_quantint8.tflite\"\n",
    "selected_images = test_images[:num_samples]\n",
    "\n",
    "# get quantization parameters\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "scale, zero_point = input_details[0]['quantization']\n",
    "\n",
    "# flatten and convert images\n",
    "flattened_test_images = selected_images.reshape(selected_images.shape[0], -1)\n",
    "int8_test_images = (flattened_test_images / scale - zero_point).astype(np.int8)\n",
    "\n",
    "# print \n",
    "with open(\"arduino/testsamples.h\", \"w\") as f:\n",
    "\n",
    "    f.write(f\"const int num_samples {num_samples} \\n\")\n",
    "    for i in range(num_samples):\n",
    "        array_name = f\"test_sample_{i}\"\n",
    "        c_array_str = convert_array_to_carray(int8_test_images[i], array_name)\n",
    "        f.write(f\"// Label: {test_labels[i]}\\n\") # to add label comment\n",
    "        f.write(c_array_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432dbac",
   "metadata": {},
   "source": [
    "**Q2: How did you create this array and what is the output?**\n",
    "1. Quantized, scale and zero-point to match TFLite model (obtained from interpreter.get_input_details())\n",
    "1. For every image:\n",
    "    1. Flatten image into 1D array\n",
    "    1. Added `f\"const signed char test_sample_{i}[{array.size}] = {{\\n  \" # to set the var name, size and type\n",
    "    1. Added each value of flattend array, joined with: \", \" # to actually add the values\n",
    "    1. Finished with \"\\n};\\n\" # finish array\n",
    "    1. Add result string to `testsamples.h` file\n",
    "\n",
    "example output for 1 array:\n",
    "```\n",
    "const signed char test_sample_4[784] = {\n",
    "    -128, -128, ...\n",
    "}\n",
    "```\n",
    "\n",
    "The size of `784` confirms we have the flattend arrays of `27x27`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5aaf36",
   "metadata": {},
   "source": [
    "## Part 3: Test the model on the Arduino\n",
    "5) Make a main file that uses the tensorflow model.h, uses the test samples (.h)\n",
    "6) Be sure to Git clone this library: https://github.com/tensorflow/tflite-micro-arduino-examples and put in under Arduino/libraries\n",
    "7) Run inference on the device, make sure to time the inference time and afterwards also report the memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c427a7d",
   "metadata": {},
   "source": [
    "\n",
    "**Q3: Copy-past the code used to perform model inference on the Arduino and how you timed your results** </br>\n",
    "\n",
    "```c\n",
    "int inference(const signed char* test_sample) {\n",
    "  // copy sample to input tensor\n",
    "  for (int i = 0; i < 784; i++) {\n",
    "    input->data.int8[i] = test_sample[i];\n",
    "  }\n",
    "  \n",
    "  // for inference time measurement\n",
    "  unsigned long start_time = micros();\n",
    "  \n",
    "  // actual inference\n",
    "  TfLiteStatus invoke_status = interpreter->Invoke();\n",
    "  \n",
    "  // end and calc inference time measurement\n",
    "  unsigned long end_time = micros();\n",
    "  unsigned long inference_time = end_time - start_time;\n",
    "  \n",
    "  if (invoke_status != kTfLiteOk) {\n",
    "    Serial.println(\"Invoke failed\");\n",
    "    return -1;\n",
    "  }\n",
    "  \n",
    "  // extracting output result\n",
    "  int8_t* output_data = output->data.int8;\n",
    "  \n",
    "  // search index with highest probability to print prediction\n",
    "  int predicted_digit = 0;\n",
    "  int8_t max_score = output_data[0];\n",
    "  \n",
    "  for (int i = 1; i < 10; i++) {\n",
    "    if (output_data[i] > max_score) {\n",
    "      predicted_digit = i;\n",
    "      max_score = output_data[i];\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // logging inference duration\n",
    "  Serial.print(\"Inference time: \");\n",
    "  Serial.print(inference_time);\n",
    "  Serial.println(\" ms\");\n",
    "  \n",
    "  return predicted_digit;\n",
    "}\n",
    "```\n",
    "&rarr; I was not sure if the timing should also include copying the input sensor and finding the actual diget so I did both.\n",
    "\n",
    "**Q4: What inference time do you get?**\n",
    "```\n",
    "Test Sample 0:\n",
    "Inference time: 268179 microseconds\n",
    "Prediction time: 268323 microseconds\n",
    "Predicted digit: 7\n",
    "\n",
    "Test Sample 1:\n",
    "Inference time: 268004 microseconds\n",
    "Prediction time: 268165 microseconds\n",
    "Predicted digit: 2\n",
    "\n",
    "Test Sample 2:\n",
    "Inference time: 268007 microseconds\n",
    "Prediction time: 268169 microseconds\n",
    "Predicted digit: 1\n",
    "\n",
    "Test Sample 3:\n",
    "Inference time: 268136 microseconds\n",
    "Prediction time: 268281 microseconds\n",
    "Predicted digit: 0\n",
    "\n",
    "Test Sample 4:\n",
    "Inference time: 268031 microseconds\n",
    "Prediction time: 268176 microseconds\n",
    "Predicted digit: 4\n",
    "```\n",
    "&rarr; So no real difference in Inference and Prediction\n",
    "\n",
    "**Q5: What memory consumption of the model on the arduino did you measure? Did you need to change anything to the allocated tensor memory to accommodate the model size?** </br>\n",
    "\n",
    "```\n",
    "Free memory before model initialization: -8041 bytes\n",
    "Input tensor dimensions: 3 dimensions with shape: [1, 28, 28]\n",
    "Free memory after model initialization: -8041 bytes\n",
    "Memory consumed by model: 69792 bytes\n",
    "```\n",
    "\n",
    "The sketch exeeced the memory of the arduino (hence the `-8041 bytes` reading), but there were no OOM errors. So I did not change anything to allocate tensor memory.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a02c81482558c89362995c8df197637e5b64523b135283abb74d4c68183e29b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.11 ('embedded-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
