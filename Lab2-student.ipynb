{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10",
   "metadata": {},
   "source": [
    "# Lab 2: Quantization of AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In this lab you will learn to **optimize** and **quantize AI models** using the **LiteRT** library (previously called Tensorflow Lite \\[For Microcontrollers]). <br />\n",
    "\n",
    "To be able to run the necessary scripts throughout this lab, you will need access to a GPU. You can either **make use of your own GPU** (through a Linux or Windows WSL system, with a GPU-enabled tensorflow installed (version 2.18.0)) **or use Google Colab**. <br />To run notebooks in colab, you will need to download the lab folder on Ufora, **unzip it and put it on your Google Drive** (this folder will only be a few MBs in size). You can **drag and drop** the unzipped folder in your Google Drive.<br /><br />\n",
    "\n",
    "\n",
    "Next, **double click on the provided .ipynb file** for each lab which will open Google Colab. <br />From there, fill in the necessary variables (such as the path to your Google Drive) and you will be able to **run and program the necessary code. Be sure te select a GPU under Runtime > Change runtime type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82319d78-c86c-4d4d-81f5-ecc1db242f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf_keras in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tf_keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.25.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (3.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf_keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf_keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade tensorflow-model-optimization\n",
    "%pip install tf_keras\n",
    "\n",
    "# Click Runtime > Restart session\n",
    "# This ensures the above installed libraries are correctly imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ab822-cf4b-4857-b808-cc616e148c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to connect your Google Drive to Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2552e552-230e-4e8a-aee6-370043ca1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your project directory\n",
    "# path_to_lab = \"drive/MyDrive/Colab Notebooks/Embedded-ML-main/\" # working on google colab\n",
    "path_to_lab = \"\" # working locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb25ac6-b0fd-43da-9173-9c1ed545032b",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below you can find **functions** which can be used to complete the lab. <br />\n",
    "_Note: when running the below code for the first time on Google Colab, you will get a warning that you need to restart your runtime session. This is expected because the kernel needs to use the expected tensorflow version._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47935bbb-4e4b-4863-87c7-b18e977abfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 14:16:33.102638: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 14:16:33.105639: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-09 14:16:33.114551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-09 14:16:33.131310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-09 14:16:33.131347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-09 14:16:33.142530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 14:16:33.824631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def mnist_model(train=False):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(6, 6), activation=tf.nn.relu, name=\"conv1\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, name=\"conv2\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, name=\"dense1\"),\n",
    "    # tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"dense2\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if train:\n",
    "        model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels))\n",
    "    else:\n",
    "        # model = tf.keras.models.load_model(\"Models/mnist.keras\")\n",
    "        model = tf.keras.models.load_model(path_to_lab + \"Models/mnist\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb714cb-dbf7-45d3-a462-edf71f1de927",
   "metadata": {},
   "source": [
    "## Quantize models using LiteRT\n",
    "\n",
    "\n",
    "### Part 1: Steps previous lab\n",
    "1) Similar to the previous lab, load the mnist dataset and pre-trained model. For this exercise we will use a pre-trained model working on the mnist dataset for digit recognition.\n",
    "2) Evaluate the model. To obtain a baseline performance, evaluate the model without any LiteRT optimizations applied.\n",
    "3) Convert the model to the LiteRT format and evaluate whether this has an impact on performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "121fdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code lab1: helper function to verify performance of tflite model\n",
    "def verify_performance(model_path):\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path_to_lab + model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # test_image = test_images[0].astype(np.float32)\n",
    "    # test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    # interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    # interpreter.invoke()\n",
    "\n",
    "    # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # predicted_label = np.argmax(output_data)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        # change type of array elements form UINT to float32\n",
    "        test_image = test_images[i].astype(np.int8)\n",
    "        # change shape of test img to be batch of lenght 1\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "        # input test_image\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "        # run model\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # get result\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        if np.argmax(output_data) == test_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(test_images)\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"TFLite Model ({model_name}) Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907bb3c4-2706-4a9f-bc9f-1af6f7096a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 14:16:35.496828: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-09 14:16:35.500850: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1746792995.875190  105850 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1746792995.875227  105850 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2025-05-09 14:16:35.875888: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-05-09 14:16:35.877223: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-05-09 14:16:35.877240: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-05-09 14:16:35.884930: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2025-05-09 14:16:35.885990: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2025-05-09 14:16:35.917180: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-05-09 14:16:35.924454: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 48571 microseconds.\n",
      "2025-05-09 14:16:35.932524: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#17 is a dynamic-sized tensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (mnist_base.tflite) Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Load pre-trained model\n",
    "model = mnist_model(train=False) # same as lab0\n",
    "# model.save(\"Models/mnist\")\n",
    "\n",
    "## Verify performance by inserting your code below\n",
    "\n",
    "# ---- see lab 0\n",
    "\n",
    "# Perform lite model conversion\n",
    "\n",
    "# -- see lab 1: Part 1, for performance difference (there was none)\n",
    "# Perform lite model conversion\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "tflite_base_model = converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/mnist_base.tflite', 'wb') as f:\n",
    "  f.write(tflite_base_model)\n",
    "\n",
    "verify_performance(path_to_lab + 'Models/mnist_base.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ba493",
   "metadata": {},
   "source": [
    "### Part 2: New steps in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f373b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to check diff in models\n",
    "def print_model_details(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of tensors: {len(interpreter.get_tensor_details())}\")\n",
    "    print(f\"Number of ops: {len(interpreter.get_signature_list())}\")\n",
    "\n",
    "    for tensor in interpreter.get_tensor_details()[0:3]:\n",
    "        print(f\"Tensor Name: {tensor['name']}, Shape: {tensor['shape']}, Type: {tensor['dtype']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def check_weight_types(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    weight_types = {tensor['dtype'] for tensor in tensor_details}\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Weight data types: {weight_types}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ad277",
   "metadata": {},
   "source": [
    "\n",
    "4) Covert the model to the LiteRT format and **quantize the model** by enabling **dynamic range** quantization. (See [here](https://ai.google.dev/edge/litert/models/post_training_quantization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03453e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746792997.446307  105850 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1746792997.446345  105850 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2025-05-09 14:16:37.446533: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-05-09 14:16:37.447908: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-05-09 14:16:37.447920: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-05-09 14:16:37.455380: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2025-05-09 14:16:37.473464: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-05-09 14:16:37.480115: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 33586 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.1\n",
    "\n",
    "# Perform Dynamic-range quantization\n",
    "dynamic_range_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "dynamic_range_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_dynamic_range_quant_model = dynamic_range_converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/dynamic_range_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_dynamic_range_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c6c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (dynamic_range_model.tflite) Accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_performance('Models/dynamic_range_model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831a792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jasper jasper  67K May  9 14:16 Models/dynamic_range_model.tflite\n",
      "-rw-rw-r-- 1 jasper jasper 248K May  9 14:16 Models/mnist_base.tflite\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [32], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant1, Shape: [64], Type: <class 'numpy.float32'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [2], Type: <class 'numpy.int32'>\n",
      "Tensor Name: arith.constant1, Shape: [], Type: <class 'numpy.int32'>\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Weight data types: {<class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Weight data types: {<class 'numpy.int8'>, <class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters compared to previous conversion\n",
    "! ls -lh Models/mnist_base.tflite Models/dynamic_range_model.tflite\n",
    "\n",
    "print_model_details(\"Models/mnist_base.tflite\")\n",
    "print_model_details(\"Models/dynamic_range_model.tflite\")\n",
    "\n",
    "check_weight_types(\"Models/mnist_base.tflite\")\n",
    "check_weight_types(\"Models/dynamic_range_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f470771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to [0,1] for float32 consistency\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc61b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746793000.172586  105850 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1746793000.172608  105850 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2025-05-09 14:16:40.172757: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-05-09 14:16:40.174008: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-05-09 14:16:40.174018: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-05-09 14:16:40.180966: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2025-05-09 14:16:40.198878: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-05-09 14:16:40.205522: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 32769 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.2\n",
    "# Perform Full int8 quantization\n",
    "\n",
    "# we need to stimate the range, i.e., (min, max) of all floating-point tensors in the model\n",
    "def representative_dataset():\n",
    "  indices = np.random.choice(len(train_images), 200, replace=False) # take evenely/randomly distrubted from data\n",
    "  for i in indices:\n",
    "      yield [np.expand_dims(train_images[i], axis=0)]\n",
    "\n",
    "# set up converter\n",
    "int8_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "int8_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_converter.representative_dataset = representative_dataset\n",
    "int8_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_quant_model = int8_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0887ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_model.tflite) Accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_performance(path_to_lab + 'Models/int8_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bc3b4",
   "metadata": {},
   "source": [
    "**Q1: Do you see any difference in accuracy? What is changed in terms of the model parameters compared to the previous conversion?**\n",
    "    \n",
    "There is no change in accuracy as it stays 0.9912.\n",
    "\n",
    "Simply comparing the size of the files we see a +- 3.5x reduction&nbsp;\n",
    "\n",
    "- 68K Mar 28 13:40 Models/dynamic_range_model.tflite\n",
    "- 248K Mar 28 13:51 Models/mnist_base.tflite\n",
    "\n",
    "Taking a closer look at the parameters we see there is no difference in amount of tensors between the models but some tensor have different datatypes in the quantized model compared to base, conversion of some tensors from float/int32 to int8 lead to this reduction in size:\n",
    "\n",
    "- Model: mnist_base.tflite \\\n",
    "Weight data types: {<class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}\n",
    "\n",
    "- Model: dynamic_range_model.tflite \\\n",
    "Weight data types: {<class 'numpy.int8'=\"\">, <class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}</class></class></class></class></class>\n",
    "    \n",
    "**Q2: Compared to dynamic range quantization, what accuracy difference do you get with full int8 precision quantization?**\n",
    "There is a slight drop (accuracy = 0.9877) if we use only 100 images to represent the data.\n",
    "\n",
    "If we use 200 images we can narrow the gap (accuracy = 0.9905).\n",
    "\n",
    "And there is an even better result if we don't naively select the first 200 images. Instead I used `np.linspace` to try and represent the whole dataset better. This resulted in a slight performance boost (accuracy = 0.9910)\n",
    "\n",
    "With random selection we can get even better results (accuracy = 0.9914), this random approach probably only works good if we use a big enough subset of the data.\n",
    "\n",
    "This makes sense since we have to try and represent the range the value's in our dataset can take so the values of S and Z get estimated with more realistic r_min and r_max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5a233",
   "metadata": {},
   "source": [
    "5) Try to train the model from scratch using **quantization-aware training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c25a7",
   "metadata": {},
   "source": [
    "Full training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f33554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training\n",
    "model = mnist_model(train=False)\n",
    "model = tf.keras.models.clone_model(model)  # we need weights from scratch\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# keep best fit\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path_to_lab + \"Models/best_fits/q_aware_model\", \n",
    "                    monitor=\"val_loss\", mode=\"min\", \n",
    "                    save_best_only=True, verbose=0)\n",
    "\n",
    "# train from scratch\n",
    "q_aware_model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ed529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:25:57.818117: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open Models/best_fits/q_aware_model: FAILED_PRECONDITION: Models/best_fits/q_aware_model; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "# reload best fit from checkpoint\n",
    "q_aware_model.load_weights(path_to_lab + \"Models/best_fits/q_aware_model\")\n",
    "# save keras model in proper file\n",
    "q_aware_model.save(path_to_lab + \"Models/qat_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0306 - accuracy: 0.9919 - 900ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db87a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# set up converter\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m int8_qat_converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_saved_model(path_to_lab \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/qat_mnist\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# path to the SavedModel directory\u001b[39;00m\n\u001b[1;32m      3\u001b[0m int8_qat_converter\u001b[38;5;241m.\u001b[39moptimizations \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOptimize\u001b[38;5;241m.\u001b[39mDEFAULT]\n\u001b[1;32m      4\u001b[0m int8_qat_converter\u001b[38;5;241m.\u001b[39mrepresentative_dataset \u001b[38;5;241m=\u001b[39m representative_dataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_mnist\") # path to the SavedModel directory\n",
    "int8_qat_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_converter.representative_dataset = representative_dataset\n",
    "int8_qat_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "int8_qat_converter.inference_input_type = tf.int8  \n",
    "int8_qat_converter.inference_output_type = tf.int8   \n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_model = int8_qat_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_model.tflite) Accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_performance('Models/int8_qat_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e798b",
   "metadata": {},
   "source": [
    "Fine tuning model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9387INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2374 - accuracy: 0.9387 - val_loss: 0.0720 - val_accuracy: 0.9863\n",
      "Epoch 2/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9849INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0893 - accuracy: 0.9850 - val_loss: 0.0433 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0486 - accuracy: 0.9889INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0486 - accuracy: 0.9889 - val_loss: 0.0412 - val_accuracy: 0.9903\n",
      "Epoch 4/10\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0445 - accuracy: 0.9890INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0445 - accuracy: 0.9890 - val_loss: 0.0379 - val_accuracy: 0.9915\n",
      "Epoch 5/10\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9896INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0410 - accuracy: 0.9896 - val_loss: 0.0376 - val_accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0404 - accuracy: 0.9896 - val_loss: 0.0415 - val_accuracy: 0.9905\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0452 - accuracy: 0.9888 - val_loss: 0.0397 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0433 - accuracy: 0.9889 - val_loss: 0.0472 - val_accuracy: 0.9882\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0418 - accuracy: 0.9890 - val_loss: 0.0382 - val_accuracy: 0.9899\n",
      "Epoch 10/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9894INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0402 - accuracy: 0.9894 - val_loss: 0.0375 - val_accuracy: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f840a1c39d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training       \n",
    "model = mnist_model(train=False)\n",
    "\n",
    "q_aware_model_fine_tune = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model_fine_tune.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# keep best fit\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path_to_lab + \"Models/best_fits/q_aware_ft_model\", \n",
    "                    monitor=\"val_loss\", mode=\"min\", \n",
    "                    save_best_only=True, verbose=0)\n",
    "\n",
    "# fine tune model\n",
    "q_aware_model_fine_tune.fit(x=train_images, y= train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf_keras.src.engine.sequential.Sequential"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q_aware_model_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:41:31.213182: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open Models/best_fits/q_aware_ft_model: FAILED_PRECONDITION: Models/best_fits/q_aware_ft_model; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_ft_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_ft_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "# reload best fit from checkpoint\n",
    "q_aware_model_fine_tune.load_weights(path_to_lab + \"Models/best_fits/q_aware_ft_model\")\n",
    "# save keras model in proper file\n",
    "q_aware_model_fine_tune.save(path_to_lab + \"Models/qat_ft_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0375 - accuracy: 0.9902 - 932ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model_fine_tune.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743176502.478601   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743176502.478623   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 16:41:42.478789: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.482437: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 16:41:42.482464: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.501723: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 16:41:42.571358: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.593971: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 115181 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_ft_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_fine_time_mnist\") # path to the SavedModel directory\n",
    "int8_qat_ft_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_ft_converter.representative_dataset = representative_dataset\n",
    "int8_qat_ft_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_ft_model = int8_qat_ft_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_ft_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_ft_model.tflite) Accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "verify_performance('Models/int8_qat_ft_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df753a",
   "metadata": {},
   "source": [
    "**Q3: What is the impact on accuracy with quantization-aware training?**\n",
    "\n",
    "If we train a quantization-aware mnist model from scratch (50 epochs):\n",
    "\n",
    "No quantization: accuracy = 0.9919\n",
    "Full int8 precision quant (int8_qat_model.tflite): accuracy = 0.9918\n",
    "\n",
    "As we can see the starting accuracy is higher then before (0.9912) but the drop after quantizing is also smaller: 0.0001 compared to 0.0002 with no quantization-aware. This is a very small difference but I think the fact that the accuracy is higher overall also makes the small drop impressive.\n",
    "\n",
    "\n",
    "If we train a mnnist model with fine-tuning (10 epochs)\"\n",
    "\n",
    "No quantization: accuracy = 0.9902\n",
    "Full int8 precision quant (int8_qat_ft_model.tflite): accuracy = 0.9915\n",
    "\n",
    "As we can see in the fine-tuned model the accuracy even goes up after applying the quantization.\n",
    "\n",
    "**Q4: When saving the tflite model, do you see any difference in the model size (full int8 quantization vs no quantization)?**\n",
    "- 70K Mar 28 16:26 Models/int8_qat_model.tflite -rw-rw-r--\n",
    "- 254K Mar 28 16:50 Models/qat_mnist.tflite\n",
    "\n",
    "&rarr; Yes we can see that the quantized model has a size reduction of +- 3.5 like before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a27c3e",
   "metadata": {},
   "source": [
    "6) **Prune** the first three layers, at 85% AND perform **full INT8 quantization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708804b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0438 - accuracy: 0.9891 - val_loss: 0.0355 - val_accuracy: 0.9915\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0401 - accuracy: 0.9897 - val_loss: 0.0409 - val_accuracy: 0.9908\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0391 - accuracy: 0.9892 - val_loss: 0.0355 - val_accuracy: 0.9919\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0429 - accuracy: 0.9891 - val_loss: 0.0334 - val_accuracy: 0.9918\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0428 - accuracy: 0.9893 - val_loss: 0.0390 - val_accuracy: 0.9904\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 0.0357 - val_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0409 - accuracy: 0.9897 - val_loss: 0.0410 - val_accuracy: 0.9903\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/mnist_pruned_85pct/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/mnist_pruned_85pct/assets\n"
     ]
    }
   ],
   "source": [
    "# code for step 6\n",
    "# Perform pruning + quantization\n",
    "\n",
    "# Step 6: Prune the first three layers at 85% and perform full INT8 quantization\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = mnist_model(train=False)\n",
    "\n",
    "# Define the layers to prune (first three trainable layers)\n",
    "layers_to_prune = [\"conv1\", \"conv2\", \"dense1\"]\n",
    "\n",
    "# Clone the model to apply pruning\n",
    "model_for_pruning = tf.keras.models.clone_model(model)\n",
    "model_for_pruning.set_weights(model.get_weights())\n",
    "\n",
    "# Apply pruning to the specified layers\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(\n",
    "        target_sparsity=0.85,\n",
    "        begin_step=0,\n",
    "        end_step=int(train_images.shape[0] / 64 * 10)  # 10 epochs\n",
    "    )\n",
    "}\n",
    "\n",
    "for layer in model_for_pruning.layers:\n",
    "    if layer.name in layers_to_prune:\n",
    "        layer = tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "model_for_pruning.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the pruned model (fine-tuning)\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='logs/pruning_85pct'),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(\n",
    "    train_images, train_labels,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Strip pruning wrappers to finalize the model\n",
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# Save the pruned model\n",
    "final_pruned_model.save(path_to_lab + 'Models/mnist_pruned_85pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b3bc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746793259.994708  105850 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1746793259.994726  105850 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2025-05-09 14:20:59.994879: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist_pruned_85pct\n",
      "2025-05-09 14:20:59.995549: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-05-09 14:20:59.995558: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: Models/mnist_pruned_85pct\n",
      "2025-05-09 14:21:00.000813: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2025-05-09 14:21:00.013979: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: Models/mnist_pruned_85pct\n",
      "2025-05-09 14:21:00.019469: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 24593 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist_pruned_85pct')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # setting input type\n",
    "converter.inference_output_type = tf.int8  # settuing output type\n",
    "\n",
    "tflite_pruned_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(path_to_lab + 'Models/mnist_pruned85_quantint8.tflite', 'wb') as f:\n",
    "    f.write(tflite_pruned_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1398bdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type FLOAT64 but expected type INT8 for input 0, name: serving_default_input_4:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Verify performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m verify_performance(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModels/mnist_pruned85_quantint8.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mverify_performance\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# input test_image\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], test_image)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# run model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[0;32m~/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=703'>704</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tensor\u001b[39m(\u001b[39mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=704'>705</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=705'>706</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=706'>707</a>\u001b[0m \u001b[39m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=717'>718</a>\u001b[0m \u001b[39m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=718'>719</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages/tensorflow/lite/python/interpreter.py?line=719'>720</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter\u001b[39m.\u001b[39mSetTensor(tensor_index, value)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type FLOAT64 but expected type INT8 for input 0, name: serving_default_input_4:0 "
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_performance('Models/mnist_pruned85_quantint8.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_model(model_path, output_zip_path=None):\n",
    "    \"\"\"\n",
    "    Zips a model file and saves it to the specified output path.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the model file to be zipped\n",
    "        output_zip_path (str, optional): Path for the output zip file. \n",
    "                        If None, uses model_path + '.zip'\n",
    "    \"\"\"\n",
    "    if output_zip_path is None:\n",
    "        output_zip_path = model_path + '.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_path, os.path.basename(model_path))\n",
    "    \n",
    "    print(f\"Model zipped to: {output_zip_path}\")\n",
    "    print(f\"Original size: {os.path.getsize(model_path)} bytes\")\n",
    "    print(f\"Zipped size: {os.path.getsize(output_zip_path)} bytes\")\n",
    "    return output_zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc810eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jasper jasper 58K Mar 28 17:28 Models/mnist_pruned85_quantint8.tflite.zip\n"
     ]
    }
   ],
   "source": [
    "! ls -lh Models/mnist_pruned85_quantint8.tflite.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08000fa",
   "metadata": {},
   "source": [
    "**Q5: Describe the observed effect in terms of accuracy and zipped model size when performing both pruning (first three layers, 85%) & full int8 quantization. (Tip: check the zipped tflite file size)**]\n",
    "\n",
    "After pruning and quantization we can decrease the size by 5x while keeping the accuracy high: 0.9897"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a02c81482558c89362995c8df197637e5b64523b135283abb74d4c68183e29b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.11 ('embedded-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
