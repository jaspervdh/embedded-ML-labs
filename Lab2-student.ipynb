{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10",
   "metadata": {},
   "source": [
    "# Lab 2: Quantization of AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In this lab you will learn to **optimize** and **quantize AI models** using the **LiteRT** library (previously called Tensorflow Lite \\[For Microcontrollers]). <br />\n",
    "\n",
    "To be able to run the necessary scripts throughout this lab, you will need access to a GPU. You can either **make use of your own GPU** (through a Linux or Windows WSL system, with a GPU-enabled tensorflow installed (version 2.18.0)) **or use Google Colab**. <br />To run notebooks in colab, you will need to download the lab folder on Ufora, **unzip it and put it on your Google Drive** (this folder will only be a few MBs in size). You can **drag and drop** the unzipped folder in your Google Drive.<br /><br />\n",
    "\n",
    "\n",
    "Next, **double click on the provided .ipynb file** for each lab which will open Google Colab. <br />From there, fill in the necessary variables (such as the path to your Google Drive) and you will be able to **run and program the necessary code. Be sure te select a GPU under Runtime > Change runtime type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82319d78-c86c-4d4d-81f5-ecc1db242f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf_keras in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tf_keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf_keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade tensorflow-model-optimization\n",
    "%pip install tf_keras\n",
    "\n",
    "# Click Runtime > Restart session\n",
    "# This ensures the above installed libraries are correctly imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ab822-cf4b-4857-b808-cc616e148c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to connect your Google Drive to Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2552e552-230e-4e8a-aee6-370043ca1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your project directory\n",
    "# path_to_lab = \"drive/MyDrive/Colab Notebooks/Embedded-ML-main/\" # working on google colab\n",
    "path_to_lab = \"\" # working locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb25ac6-b0fd-43da-9173-9c1ed545032b",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below you can find **functions** which can be used to complete the lab. <br />\n",
    "_Note: when running the below code for the first time on Google Colab, you will get a warning that you need to restart your runtime session. This is expected because the kernel needs to use the expected tensorflow version._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47935bbb-4e4b-4863-87c7-b18e977abfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 15:06:31.394985: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 15:06:31.398550: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 15:06:31.408574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743170791.422490   27551 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743170791.426430   27551 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 15:06:31.441827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def mnist_model(train=False):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(6, 6), activation=tf.nn.relu, name=\"conv1\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, name=\"conv2\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, name=\"dense1\"),\n",
    "    # tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"dense2\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if train:\n",
    "        model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels))\n",
    "    else:\n",
    "        # model = tf.keras.models.load_model(\"Models/mnist.keras\")\n",
    "        model = tf.keras.models.load_model(path_to_lab + \"Models/mnist\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb714cb-dbf7-45d3-a462-edf71f1de927",
   "metadata": {},
   "source": [
    "## Quantize models using LiteRT\n",
    "\n",
    "\n",
    "### Part 1: Steps previous lab\n",
    "1) Similar to the previous lab, load the mnist dataset and pre-trained model. For this exercise we will use a pre-trained model working on the mnist dataset for digit recognition.\n",
    "2) Evaluate the model. To obtain a baseline performance, evaluate the model without any LiteRT optimizations applied.\n",
    "3) Convert the model to the LiteRT format and evaluate whether this has an impact on performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121fdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code lab1: helper function to verify performance of tflite model\n",
    "def verify_peformance(model_path):\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path_to_lab + model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # test_image = test_images[0].astype(np.float32)\n",
    "    # test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    # interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    # interpreter.invoke()\n",
    "\n",
    "    # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # predicted_label = np.argmax(output_data)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        # change type of array elements form UINT to float32\n",
    "        test_image = test_images[i].astype(np.float32)\n",
    "        # change shape of test img to be batch of lenght 1\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "        # input test_image\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "        # run model\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # get result\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        if np.argmax(output_data) == test_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(test_images)\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"TFLite Model ({model_name}) Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907bb3c4-2706-4a9f-bc9f-1af6f7096a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1743170815.885128   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743170815.885152   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 15:06:55.885872: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 15:06:55.886847: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 15:06:55.886869: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "I0000 00:00:1743170815.891115   27551 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-03-28 15:06:55.891958: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 15:06:55.912915: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 15:06:55.920152: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 34291 microseconds.\n",
      "2025-03-28 15:06:55.928270: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (mnist_base.tflite) Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Load pre-trained model\n",
    "model = mnist_model(train=False) # same as lab0\n",
    "# model.save(\"Models/mnist\")\n",
    "\n",
    "## Verify performance by inserting your code below\n",
    "\n",
    "# ---- see lab 0\n",
    "\n",
    "# Perform lite model conversion\n",
    "\n",
    "# -- see lab 1: Part 1, for performance difference (there was none)\n",
    "# Perform lite model conversion\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "tflite_base_model = converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/mnist_base.tflite', 'wb') as f:\n",
    "  f.write(tflite_base_model)\n",
    "\n",
    "verify_peformance(path_to_lab + 'Models/mnist_base.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ba493",
   "metadata": {},
   "source": [
    "### Part 2: New steps in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f373b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to check diff in models\n",
    "def print_model_details(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of tensors: {len(interpreter.get_tensor_details())}\")\n",
    "    print(f\"Number of ops: {len(interpreter.get_signature_list())}\")\n",
    "\n",
    "    for tensor in interpreter.get_tensor_details()[0:3]:\n",
    "        print(f\"Tensor Name: {tensor['name']}, Shape: {tensor['shape']}, Type: {tensor['dtype']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def check_weight_types(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    weight_types = {tensor['dtype'] for tensor in tensor_details}\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Weight data types: {weight_types}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ad277",
   "metadata": {},
   "source": [
    "\n",
    "4) Covert the model to the LiteRT format and **quantize the model** by enabling **dynamic range** quantization. (See [here](https://ai.google.dev/edge/litert/models/post_training_quantization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03453e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743165629.923512    6279 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743165629.923539    6279 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 13:40:29.923737: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 13:40:29.924987: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 13:40:29.925006: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-03-28 13:40:29.931488: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 13:40:29.951106: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 13:40:29.958324: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 34593 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.1\n",
    "\n",
    "# Perform Dynamic-range quantization\n",
    "dynamic_range_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "dynamic_range_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_dynamic_range_quant_model = dynamic_range_converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/dynamic_range_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_dynamic_range_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c6c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (dynamic_range_model.tflite) Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance('Models/dynamic_range_model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "831a792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jasper jasper  68K Mar 28 13:40 Models/dynamic_range_model.tflite\n",
      "-rw-rw-r-- 1 jasper jasper 248K Mar 28 13:51 Models/mnist_base.tflite\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [32], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant1, Shape: [64], Type: <class 'numpy.float32'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [2], Type: <class 'numpy.int32'>\n",
      "Tensor Name: arith.constant1, Shape: [], Type: <class 'numpy.int32'>\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Weight data types: {<class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Weight data types: {<class 'numpy.int8'>, <class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters compared to previous conversion\n",
    "! ls -lh Models/mnist_base.tflite Models/dynamic_range_model.tflite\n",
    "\n",
    "print_model_details(\"Models/mnist_base.tflite\")\n",
    "print_model_details(\"Models/dynamic_range_model.tflite\")\n",
    "\n",
    "check_weight_types(\"Models/mnist_base.tflite\")\n",
    "check_weight_types(\"Models/dynamic_range_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f470771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to [0,1] for float32 consistency\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc61b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743172453.576107   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743172453.576135   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 15:34:13.576361: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 15:34:13.578140: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 15:34:13.578162: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-03-28 15:34:13.585509: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 15:34:13.609302: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 15:34:13.620294: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 43935 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.2\n",
    "# Perform Full int8 quantization\n",
    "\n",
    "# we need to stimate the range, i.e., (min, max) of all floating-point tensors in the model\n",
    "def representative_dataset():\n",
    "  indices = np.random.choice(len(train_images), 200, replace=False) # take evenely/randomly distrubted from data\n",
    "  for i in indices:\n",
    "      yield [np.expand_dims(train_images[i], axis=0)]\n",
    "\n",
    "# set up converter\n",
    "int8_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "int8_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_converter.representative_dataset = representative_dataset\n",
    "int8_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_quant_model = int8_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db0887ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_model.tflite) Accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance(path_to_lab + 'Models/int8_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bc3b4",
   "metadata": {},
   "source": [
    "**Q1: Do you see any difference in accuracy? What is changed in terms of the model parameters compared to the previous conversion?**\n",
    "    \n",
    "There is no change in accuracy as it stays 0.9912.\n",
    "\n",
    "Simply comparing the size of the files we see a +- 3.5x reduction&nbsp;\n",
    "\n",
    "- 68K Mar 28 13:40 Models/dynamic_range_model.tflite\n",
    "- 248K Mar 28 13:51 Models/mnist_base.tflite\n",
    "\n",
    "Taking a closer look at the parameters we see there is no difference in amount of tensors between the models but some tensor have different datatypes in the quantized model compared to base, conversion of some tensors from float/int32 to int8 lead to this reduction in size:\n",
    "\n",
    "- Model: mnist_base.tflite \\\n",
    "Weight data types: {<class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}\n",
    "\n",
    "- Model: dynamic_range_model.tflite \\\n",
    "Weight data types: {<class 'numpy.int8'=\"\">, <class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}</class></class></class></class></class>\n",
    "    \n",
    "**Q2: Compared to dynamic range quantization, what accuracy difference do you get with full int8 precision quantization?**\n",
    "There is a slight drop (accuracy = 0.9877) if we use only 100 images to represent the data.\n",
    "\n",
    "If we use 200 images we can narrow the gap (accuracy = 0.9905).\n",
    "\n",
    "And there is an even better result if we don't naively select the first 200 images. Instead I used `np.linspace` to try and represent the whole dataset better. This resulted in a slight performance boost (accuracy = 0.9910)\n",
    "\n",
    "With random selection we can get even better results (accuracy = 0.9914), this random approach probably only works good if we use a big enough subset of the data.\n",
    "\n",
    "This makes sense since we have to try and represent the range the value's in our dataset can take so the values of S and Z get estimated with more realistic r_min and r_max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5a233",
   "metadata": {},
   "source": [
    "5) Try to train the model from scratch using **quantization-aware training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c25a7",
   "metadata": {},
   "source": [
    "Full training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f33554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 1.9020 - accuracy: 0.3141\n",
      "Epoch 1: val_loss improved from inf to 1.86321, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 19ms/step - loss: 1.9020 - accuracy: 0.3141 - val_loss: 1.8632 - val_accuracy: 0.2813\n",
      "Epoch 2/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 1.2534 - accuracy: 0.5445\n",
      "Epoch 2: val_loss improved from 1.86321 to 0.66732, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 20s 21ms/step - loss: 1.2526 - accuracy: 0.5448 - val_loss: 0.6673 - val_accuracy: 0.7775\n",
      "Epoch 3/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.5264 - accuracy: 0.8372\n",
      "Epoch 3: val_loss improved from 0.66732 to 0.19571, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 20ms/step - loss: 0.5260 - accuracy: 0.8373 - val_loss: 0.1957 - val_accuracy: 0.9555\n",
      "Epoch 4/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9475\n",
      "Epoch 4: val_loss improved from 0.19571 to 0.09594, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 18s 20ms/step - loss: 0.2014 - accuracy: 0.9475 - val_loss: 0.0959 - val_accuracy: 0.9753\n",
      "Epoch 5/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.9635\n",
      "Epoch 5: val_loss improved from 0.09594 to 0.08341, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 20ms/step - loss: 0.1362 - accuracy: 0.9635 - val_loss: 0.0834 - val_accuracy: 0.9787\n",
      "Epoch 6/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9680\n",
      "Epoch 6: val_loss improved from 0.08341 to 0.06386, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 16s 17ms/step - loss: 0.1161 - accuracy: 0.9681 - val_loss: 0.0639 - val_accuracy: 0.9815\n",
      "Epoch 7/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9712\n",
      "Epoch 7: val_loss improved from 0.06386 to 0.06263, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 17s 18ms/step - loss: 0.1050 - accuracy: 0.9712 - val_loss: 0.0626 - val_accuracy: 0.9823\n",
      "Epoch 8/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9725\n",
      "Epoch 8: val_loss improved from 0.06263 to 0.05508, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 16s 18ms/step - loss: 0.0974 - accuracy: 0.9725 - val_loss: 0.0551 - val_accuracy: 0.9841\n",
      "Epoch 9/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0926 - accuracy: 0.9742\n",
      "Epoch 9: val_loss did not improve from 0.05508\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0927 - accuracy: 0.9742 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 10/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9749\n",
      "Epoch 10: val_loss did not improve from 0.05508\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0915 - accuracy: 0.9749 - val_loss: 0.0600 - val_accuracy: 0.9837\n",
      "Epoch 11/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9760\n",
      "Epoch 11: val_loss did not improve from 0.05508\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.0823 - accuracy: 0.9760 - val_loss: 0.0567 - val_accuracy: 0.9849\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9779\n",
      "Epoch 12: val_loss improved from 0.05508 to 0.05107, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0816 - accuracy: 0.9779 - val_loss: 0.0511 - val_accuracy: 0.9861\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9780\n",
      "Epoch 13: val_loss did not improve from 0.05107\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0783 - accuracy: 0.9780 - val_loss: 0.0524 - val_accuracy: 0.9866\n",
      "Epoch 14/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9776\n",
      "Epoch 14: val_loss did not improve from 0.05107\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.0514 - val_accuracy: 0.9862\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9790\n",
      "Epoch 15: val_loss improved from 0.05107 to 0.04354, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0749 - accuracy: 0.9790 - val_loss: 0.0435 - val_accuracy: 0.9879\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9806\n",
      "Epoch 16: val_loss did not improve from 0.04354\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0701 - accuracy: 0.9806 - val_loss: 0.0479 - val_accuracy: 0.9862\n",
      "Epoch 17/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9797\n",
      "Epoch 17: val_loss improved from 0.04354 to 0.04111, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0687 - accuracy: 0.9797 - val_loss: 0.0411 - val_accuracy: 0.9882\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9817\n",
      "Epoch 18: val_loss did not improve from 0.04111\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0630 - accuracy: 0.9817 - val_loss: 0.0425 - val_accuracy: 0.9873\n",
      "Epoch 19/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9814\n",
      "Epoch 19: val_loss did not improve from 0.04111\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.0476 - val_accuracy: 0.9875\n",
      "Epoch 20/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9821\n",
      "Epoch 20: val_loss improved from 0.04111 to 0.03692, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 22s 24ms/step - loss: 0.0627 - accuracy: 0.9821 - val_loss: 0.0369 - val_accuracy: 0.9901\n",
      "Epoch 21/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9834\n",
      "Epoch 21: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0579 - accuracy: 0.9834 - val_loss: 0.0406 - val_accuracy: 0.9879\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9816\n",
      "Epoch 22: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0655 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9877\n",
      "Epoch 23/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9829\n",
      "Epoch 23: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0617 - accuracy: 0.9829 - val_loss: 0.0405 - val_accuracy: 0.9882\n",
      "Epoch 24/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9830\n",
      "Epoch 24: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0618 - accuracy: 0.9830 - val_loss: 0.0426 - val_accuracy: 0.9888\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9831\n",
      "Epoch 25: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0615 - accuracy: 0.9831 - val_loss: 0.0447 - val_accuracy: 0.9876\n",
      "Epoch 26/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9840\n",
      "Epoch 26: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0558 - accuracy: 0.9840 - val_loss: 0.0448 - val_accuracy: 0.9879\n",
      "Epoch 27/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0573 - accuracy: 0.9834\n",
      "Epoch 27: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9873\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9845\n",
      "Epoch 28: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0548 - accuracy: 0.9845 - val_loss: 0.0441 - val_accuracy: 0.9882\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9840\n",
      "Epoch 29: val_loss did not improve from 0.03692\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0561 - accuracy: 0.9840 - val_loss: 0.0446 - val_accuracy: 0.9880\n",
      "Epoch 30/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9849\n",
      "Epoch 30: val_loss improved from 0.03692 to 0.03637, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0556 - accuracy: 0.9849 - val_loss: 0.0364 - val_accuracy: 0.9906\n",
      "Epoch 31/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9855\n",
      "Epoch 31: val_loss improved from 0.03637 to 0.03285, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0495 - accuracy: 0.9855 - val_loss: 0.0328 - val_accuracy: 0.9911\n",
      "Epoch 32/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9848\n",
      "Epoch 32: val_loss did not improve from 0.03285\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0554 - accuracy: 0.9849 - val_loss: 0.0447 - val_accuracy: 0.9878\n",
      "Epoch 33/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9838\n",
      "Epoch 33: val_loss did not improve from 0.03285\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.0380 - val_accuracy: 0.9889\n",
      "Epoch 34/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9846\n",
      "Epoch 34: val_loss did not improve from 0.03285\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0548 - accuracy: 0.9846 - val_loss: 0.0336 - val_accuracy: 0.9894\n",
      "Epoch 35/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9852\n",
      "Epoch 35: val_loss did not improve from 0.03285\n",
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0533 - accuracy: 0.9851 - val_loss: 0.0335 - val_accuracy: 0.9914\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9851\n",
      "Epoch 36: val_loss improved from 0.03285 to 0.03184, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0527 - accuracy: 0.9851 - val_loss: 0.0318 - val_accuracy: 0.9910\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9852\n",
      "Epoch 37: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0538 - accuracy: 0.9852 - val_loss: 0.0394 - val_accuracy: 0.9893\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9855\n",
      "Epoch 38: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0512 - accuracy: 0.9855 - val_loss: 0.0363 - val_accuracy: 0.9899\n",
      "Epoch 39/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9869\n",
      "Epoch 39: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0466 - accuracy: 0.9869 - val_loss: 0.0380 - val_accuracy: 0.9896\n",
      "Epoch 40/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9858\n",
      "Epoch 40: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0515 - accuracy: 0.9858 - val_loss: 0.0342 - val_accuracy: 0.9910\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9861\n",
      "Epoch 41: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0498 - accuracy: 0.9861 - val_loss: 0.0415 - val_accuracy: 0.9885\n",
      "Epoch 42/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0505 - accuracy: 0.9857\n",
      "Epoch 42: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0504 - accuracy: 0.9857 - val_loss: 0.0382 - val_accuracy: 0.9900\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9864\n",
      "Epoch 43: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0482 - accuracy: 0.9864 - val_loss: 0.0371 - val_accuracy: 0.9902\n",
      "Epoch 44/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9866\n",
      "Epoch 44: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0448 - accuracy: 0.9867 - val_loss: 0.0365 - val_accuracy: 0.9911\n",
      "Epoch 45/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9865\n",
      "Epoch 45: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0494 - accuracy: 0.9865 - val_loss: 0.0349 - val_accuracy: 0.9901\n",
      "Epoch 46/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0507 - accuracy: 0.9857\n",
      "Epoch 46: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0507 - accuracy: 0.9857 - val_loss: 0.0324 - val_accuracy: 0.9901\n",
      "Epoch 47/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.9866\n",
      "Epoch 47: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0480 - accuracy: 0.9866 - val_loss: 0.0396 - val_accuracy: 0.9895\n",
      "Epoch 48/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9866\n",
      "Epoch 48: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0494 - accuracy: 0.9866 - val_loss: 0.0343 - val_accuracy: 0.9900\n",
      "Epoch 49/50\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9876\n",
      "Epoch 49: val_loss did not improve from 0.03184\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0449 - accuracy: 0.9876 - val_loss: 0.0373 - val_accuracy: 0.9903\n",
      "Epoch 50/50\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9873\n",
      "Epoch 50: val_loss improved from 0.03184 to 0.03065, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0452 - accuracy: 0.9873 - val_loss: 0.0306 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f84323e6d90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training\n",
    "model = mnist_model(train=False)\n",
    "model = tf.keras.models.clone_model(model)  # we need weights from scratch\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# keep best fit\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path_to_lab + \"Models/best_fits/q_aware_model\", \n",
    "                    monitor=\"val_loss\", mode=\"min\", \n",
    "                    save_best_only=True, verbose=0)\n",
    "\n",
    "# train from scratch\n",
    "q_aware_model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "538ed529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:25:57.818117: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open Models/best_fits/q_aware_model: FAILED_PRECONDITION: Models/best_fits/q_aware_model; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "# reload best fit from checkpoint\n",
    "q_aware_model.load_weights(path_to_lab + \"Models/best_fits/q_aware_model\")\n",
    "# save keras model in proper file\n",
    "q_aware_model.save(path_to_lab + \"Models/qat_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4066895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0306 - accuracy: 0.9919 - 900ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0db87a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743175561.524383   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743175561.524402   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 16:26:01.524581: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/qat_mnist\n",
      "2025-03-28 16:26:01.528865: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 16:26:01.528890: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/qat_mnist\n",
      "2025-03-28 16:26:01.545038: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 16:26:01.612096: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/qat_mnist\n",
      "2025-03-28 16:26:01.634320: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 109742 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_mnist\") # path to the SavedModel directory\n",
    "int8_qat_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_converter.representative_dataset = representative_dataset\n",
    "int8_qat_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_model = int8_qat_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d381b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_model.tflite) Accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance('Models/int8_qat_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e798b",
   "metadata": {},
   "source": [
    "Fine tuning model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f93f6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.2377 - accuracy: 0.9387INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 17s 18ms/step - loss: 0.2374 - accuracy: 0.9387 - val_loss: 0.0720 - val_accuracy: 0.9863\n",
      "Epoch 2/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9849INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0893 - accuracy: 0.9850 - val_loss: 0.0433 - val_accuracy: 0.9907\n",
      "Epoch 3/10\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0486 - accuracy: 0.9889INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0486 - accuracy: 0.9889 - val_loss: 0.0412 - val_accuracy: 0.9903\n",
      "Epoch 4/10\n",
      "936/938 [============================>.] - ETA: 0s - loss: 0.0445 - accuracy: 0.9890INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0445 - accuracy: 0.9890 - val_loss: 0.0379 - val_accuracy: 0.9915\n",
      "Epoch 5/10\n",
      "935/938 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9896INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0410 - accuracy: 0.9896 - val_loss: 0.0376 - val_accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0404 - accuracy: 0.9896 - val_loss: 0.0415 - val_accuracy: 0.9905\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0452 - accuracy: 0.9888 - val_loss: 0.0397 - val_accuracy: 0.9902\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0433 - accuracy: 0.9889 - val_loss: 0.0472 - val_accuracy: 0.9882\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0418 - accuracy: 0.9890 - val_loss: 0.0382 - val_accuracy: 0.9899\n",
      "Epoch 10/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9894INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_ft_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0402 - accuracy: 0.9894 - val_loss: 0.0375 - val_accuracy: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f840a1c39d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training\n",
    "model = mnist_model(train=False)\n",
    "\n",
    "q_aware_model_fine_tune = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model_fine_tune.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# keep best fit\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path_to_lab + \"Models/best_fits/q_aware_ft_model\", \n",
    "                    monitor=\"val_loss\", mode=\"min\", \n",
    "                    save_best_only=True, verbose=0)\n",
    "\n",
    "# fine tune model\n",
    "q_aware_model_fine_tune.fit(x=train_images, y= train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a686ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf_keras.src.engine.sequential.Sequential"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q_aware_model_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a56f81b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:41:31.213182: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open Models/best_fits/q_aware_ft_model: FAILED_PRECONDITION: Models/best_fits/q_aware_ft_model; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_ft_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_ft_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "# reload best fit from checkpoint\n",
    "q_aware_model_fine_tune.load_weights(path_to_lab + \"Models/best_fits/q_aware_ft_model\")\n",
    "# save keras model in proper file\n",
    "q_aware_model_fine_tune.save(path_to_lab + \"Models/qat_ft_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bbd4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0375 - accuracy: 0.9902 - 932ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model_fine_tune.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7c3eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743176502.478601   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743176502.478623   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 16:41:42.478789: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.482437: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 16:41:42.482464: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.501723: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 16:41:42.571358: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/qat_fine_time_mnist\n",
      "2025-03-28 16:41:42.593971: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 115181 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_ft_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_fine_time_mnist\") # path to the SavedModel directory\n",
    "int8_qat_ft_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_ft_converter.representative_dataset = representative_dataset\n",
    "int8_qat_ft_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_ft_model = int8_qat_ft_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_ft_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "989c8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_ft_model.tflite) Accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "verify_peformance('Models/int8_qat_ft_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df753a",
   "metadata": {},
   "source": [
    "**Q3: What is the impact on accuracy with quantization-aware training?**\n",
    "\n",
    "If we train a quantization-aware mnist model from scratch (50 epochs):\n",
    "\n",
    "No quantization: accuracy = 0.9919\n",
    "Full int8 precision quant (int8_qat_model.tflite): accuracy = 0.9918\n",
    "\n",
    "As we can see the starting accuracy is higher then before (0.9912) but the drop after quantizing is also smaller: 0.0001 compared to 0.0002 with no quantization-aware. This is a very small difference but I think the fact that the accuracy is higher overall also makes the small drop impressive.\n",
    "\n",
    "\n",
    "If we train a mnnist model with fine-tuning (10 epochs)\"\n",
    "\n",
    "No quantization: accuracy = 0.9902\n",
    "Full int8 precision quant (int8_qat_ft_model.tflite): accuracy = 0.9915\n",
    "\n",
    "As we can see in the fine-tuned model the accuracy even goes up after applying the quantization.\n",
    "\n",
    "**Q4: When saving the tflite model, do you see any difference in the model size (full int8 quantization vs no quantization)?**\n",
    "- 70K Mar 28 16:26 Models/int8_qat_model.tflite -rw-rw-r--\n",
    "- 254K Mar 28 16:50 Models/qat_mnist.tflite\n",
    "\n",
    "&rarr; Yes we can see that the quantized model has a size reduction of +- 3.5 like before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a27c3e",
   "metadata": {},
   "source": [
    "6) **Prune** the first three layers, at 85% AND perform **full INT8 quantization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "708804b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0438 - accuracy: 0.9891 - val_loss: 0.0355 - val_accuracy: 0.9915\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0401 - accuracy: 0.9897 - val_loss: 0.0409 - val_accuracy: 0.9908\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0391 - accuracy: 0.9892 - val_loss: 0.0355 - val_accuracy: 0.9919\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0429 - accuracy: 0.9891 - val_loss: 0.0334 - val_accuracy: 0.9918\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0428 - accuracy: 0.9893 - val_loss: 0.0390 - val_accuracy: 0.9904\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 0.0357 - val_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0409 - accuracy: 0.9897 - val_loss: 0.0410 - val_accuracy: 0.9903\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/mnist_pruned_85pct/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/mnist_pruned_85pct/assets\n"
     ]
    }
   ],
   "source": [
    "# code for step 6\n",
    "# Perform pruning + quantization\n",
    "\n",
    "# Step 6: Prune the first three layers at 85% and perform full INT8 quantization\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = mnist_model(train=False)\n",
    "\n",
    "# Define the layers to prune (first three trainable layers)\n",
    "layers_to_prune = [\"conv1\", \"conv2\", \"dense1\"]\n",
    "\n",
    "# Clone the model to apply pruning\n",
    "model_for_pruning = tf.keras.models.clone_model(model)\n",
    "model_for_pruning.set_weights(model.get_weights())\n",
    "\n",
    "# Apply pruning to the specified layers\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(\n",
    "        target_sparsity=0.85,\n",
    "        begin_step=0,\n",
    "        end_step=int(train_images.shape[0] / 64 * 10)  # 10 epochs\n",
    "    )\n",
    "}\n",
    "\n",
    "for layer in model_for_pruning.layers:\n",
    "    if layer.name in layers_to_prune:\n",
    "        layer = tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "model_for_pruning.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the pruned model (fine-tuning)\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='logs/pruning_85pct'),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(\n",
    "    train_images, train_labels,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Strip pruning wrappers to finalize the model\n",
    "final_pruned_model = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# Save the pruned model\n",
    "final_pruned_model.save(path_to_lab + 'Models/mnist_pruned_85pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b3bc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743178481.168723   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743178481.168747   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 17:14:41.168998: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist_pruned_85pct\n",
      "2025-03-28 17:14:41.170754: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 17:14:41.170772: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist_pruned_85pct\n",
      "2025-03-28 17:14:41.178246: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 17:14:41.195155: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist_pruned_85pct\n",
      "2025-03-28 17:14:41.203829: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 34838 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist_pruned_85pct')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.int8  # setting input type\n",
    "# converter.inference_output_type = tf.int8  # settuing output type\n",
    "\n",
    "tflite_pruned_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(path_to_lab + 'Models/mnist_pruned85_quantint8.tflite', 'wb') as f:\n",
    "    f.write(tflite_pruned_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1398bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (mnist_pruned85_quantint8.tflite) Accuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance('Models/mnist_pruned85_quantint8.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae73a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_model(model_path, output_zip_path=None):\n",
    "    \"\"\"\n",
    "    Zips a model file and saves it to the specified output path.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the model file to be zipped\n",
    "        output_zip_path (str, optional): Path for the output zip file. \n",
    "                        If None, uses model_path + '.zip'\n",
    "    \"\"\"\n",
    "    if output_zip_path is None:\n",
    "        output_zip_path = model_path + '.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_path, os.path.basename(model_path))\n",
    "    \n",
    "    print(f\"Model zipped to: {output_zip_path}\")\n",
    "    print(f\"Original size: {os.path.getsize(model_path)} bytes\")\n",
    "    print(f\"Zipped size: {os.path.getsize(output_zip_path)} bytes\")\n",
    "    return output_zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3cc810eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jasper jasper 58K Mar 28 17:28 Models/mnist_pruned85_quantint8.tflite.zip\n"
     ]
    }
   ],
   "source": [
    "! ls -lh Models/mnist_pruned85_quantint8.tflite.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08000fa",
   "metadata": {},
   "source": [
    "**Q5: Describe the observed effect in terms of accuracy and zipped model size when performing both pruning (first three layers, 85%) & full int8 quantization. (Tip: check the zipped tflite file size)**]\n",
    "\n",
    "After pruning and quantization we can decrease the size by 5x while keeping the accuracy high: 0.9897"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a02c81482558c89362995c8df197637e5b64523b135283abb74d4c68183e29b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.11 ('embedded-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
