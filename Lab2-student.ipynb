{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761bb758-3eed-4ce9-a6c5-8107840d8b10",
   "metadata": {},
   "source": [
    "# Lab 2: Quantization of AI models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dac44-0c8a-4250-80d9-c45a8d74ffe9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "In this lab you will learn to **optimize** and **quantize AI models** using the **LiteRT** library (previously called Tensorflow Lite \\[For Microcontrollers]). <br />\n",
    "\n",
    "To be able to run the necessary scripts throughout this lab, you will need access to a GPU. You can either **make use of your own GPU** (through a Linux or Windows WSL system, with a GPU-enabled tensorflow installed (version 2.18.0)) **or use Google Colab**. <br />To run notebooks in colab, you will need to download the lab folder on Ufora, **unzip it and put it on your Google Drive** (this folder will only be a few MBs in size). You can **drag and drop** the unzipped folder in your Google Drive.<br /><br />\n",
    "\n",
    "\n",
    "Next, **double click on the provided .ipynb file** for each lab which will open Google Colab. <br />From there, fill in the necessary variables (such as the path to your Google Drive) and you will be able to **run and program the necessary code. Be sure te select a GPU under Runtime > Change runtime type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82319d78-c86c-4d4d-81f5-ecc1db242f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (0.1.9)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.26.4)\n",
      "Requirement already satisfied: six~=1.14 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow-model-optimization) (1.16.0)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (25.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from dm-tree~=0.1.1->tensorflow-model-optimization) (1.17.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf_keras in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tf_keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf_keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf_keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jasper/miniconda3/envs/embedded-ml/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf_keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade tensorflow-model-optimization\n",
    "%pip install tf_keras\n",
    "\n",
    "# Click Runtime > Restart session\n",
    "# This ensures the above installed libraries are correctly imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ab822-cf4b-4857-b808-cc616e148c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to connect your Google Drive to Colab\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2552e552-230e-4e8a-aee6-370043ca1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your project directory\n",
    "# path_to_lab = \"drive/MyDrive/Colab Notebooks/Embedded-ML-main/\" # working on google colab\n",
    "path_to_lab = \"\" # working locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb25ac6-b0fd-43da-9173-9c1ed545032b",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Below you can find **functions** which can be used to complete the lab. <br />\n",
    "_Note: when running the below code for the first time on Google Colab, you will get a warning that you need to restart your runtime session. This is expected because the kernel needs to use the expected tensorflow version._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47935bbb-4e4b-4863-87c7-b18e977abfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 15:06:31.394985: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 15:06:31.398550: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-28 15:06:31.408574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743170791.422490   27551 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743170791.426430   27551 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 15:06:31.441827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def mnist_model(train=False):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(6, 6), activation=tf.nn.relu, name=\"conv1\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu, name=\"conv2\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu, name=\"dense1\"),\n",
    "    # tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name=\"dense2\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if train:\n",
    "        model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels))\n",
    "    else:\n",
    "        # model = tf.keras.models.load_model(\"Models/mnist.keras\")\n",
    "        model = tf.keras.models.load_model(path_to_lab + \"Models/mnist\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb714cb-dbf7-45d3-a462-edf71f1de927",
   "metadata": {},
   "source": [
    "## Quantize models using LiteRT\n",
    "\n",
    "\n",
    "### Part 1: Steps previous lab\n",
    "1) Similar to the previous lab, load the mnist dataset and pre-trained model. For this exercise we will use a pre-trained model working on the mnist dataset for digit recognition.\n",
    "2) Evaluate the model. To obtain a baseline performance, evaluate the model without any LiteRT optimizations applied.\n",
    "3) Convert the model to the LiteRT format and evaluate whether this has an impact on performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121fdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code lab1: helper function to verify performance of tflite model\n",
    "def verify_peformance(model_path):\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path_to_lab + model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test model on random input data.\n",
    "    # input_shape = input_details[0]['shape']\n",
    "    # test_image = test_images[0].astype(np.float32)\n",
    "    # test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    # interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    # interpreter.invoke()\n",
    "\n",
    "    # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    # predicted_label = np.argmax(output_data)\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        # change type of array elements form UINT to float32\n",
    "        test_image = test_images[i].astype(np.float32)\n",
    "        # change shape of test img to be batch of lenght 1\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "        # input test_image\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "        # run model\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # get result\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        if np.argmax(output_data) == test_labels[i]:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(test_images)\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"TFLite Model ({model_name}) Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907bb3c4-2706-4a9f-bc9f-1af6f7096a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1743170815.885128   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743170815.885152   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 15:06:55.885872: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 15:06:55.886847: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 15:06:55.886869: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "I0000 00:00:1743170815.891115   27551 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-03-28 15:06:55.891958: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 15:06:55.912915: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 15:06:55.920152: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 34291 microseconds.\n",
      "2025-03-28 15:06:55.928270: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (mnist_base.tflite) Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Load pre-trained model\n",
    "model = mnist_model(train=False) # same as lab0\n",
    "# model.save(\"Models/mnist\")\n",
    "\n",
    "## Verify performance by inserting your code below\n",
    "\n",
    "# ---- see lab 0\n",
    "\n",
    "# Perform lite model conversion\n",
    "\n",
    "# -- see lab 1: Part 1, for performance difference (there was none)\n",
    "# Perform lite model conversion\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "tflite_base_model = converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/mnist_base.tflite', 'wb') as f:\n",
    "  f.write(tflite_base_model)\n",
    "\n",
    "verify_peformance(path_to_lab + 'Models/mnist_base.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ba493",
   "metadata": {},
   "source": [
    "### Part 2: New steps in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f373b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to check diff in models\n",
    "def print_model_details(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of tensors: {len(interpreter.get_tensor_details())}\")\n",
    "    print(f\"Number of ops: {len(interpreter.get_signature_list())}\")\n",
    "\n",
    "    for tensor in interpreter.get_tensor_details()[0:3]:\n",
    "        print(f\"Tensor Name: {tensor['name']}, Shape: {tensor['shape']}, Type: {tensor['dtype']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def check_weight_types(model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "    weight_types = {tensor['dtype'] for tensor in tensor_details}\n",
    "\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Weight data types: {weight_types}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ad277",
   "metadata": {},
   "source": [
    "\n",
    "4) Covert the model to the LiteRT format and **quantize the model** by enabling **dynamic range** quantization. (See [here](https://ai.google.dev/edge/litert/models/post_training_quantization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03453e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743165629.923512    6279 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743165629.923539    6279 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 13:40:29.923737: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 13:40:29.924987: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 13:40:29.925006: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-03-28 13:40:29.931488: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 13:40:29.951106: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 13:40:29.958324: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 34593 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.1\n",
    "\n",
    "# Perform Dynamic-range quantization\n",
    "dynamic_range_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "dynamic_range_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_dynamic_range_quant_model = dynamic_range_converter.convert()\n",
    "\n",
    "with open(path_to_lab + 'Models/dynamic_range_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_dynamic_range_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c6c7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (dynamic_range_model.tflite) Accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance('Models/dynamic_range_model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "831a792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 jasper jasper  68K Mar 28 13:40 Models/dynamic_range_model.tflite\n",
      "-rw-rw-r-- 1 jasper jasper 248K Mar 28 13:51 Models/mnist_base.tflite\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [32], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant1, Shape: [64], Type: <class 'numpy.float32'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Number of tensors: 25\n",
      "Number of ops: 1\n",
      "Tensor Name: serving_default_input_4:0, Shape: [ 1 28 28], Type: <class 'numpy.float32'>\n",
      "Tensor Name: arith.constant, Shape: [2], Type: <class 'numpy.int32'>\n",
      "Tensor Name: arith.constant1, Shape: [], Type: <class 'numpy.int32'>\n",
      "\n",
      "\n",
      "Model: mnist_base.tflite\n",
      "Weight data types: {<class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n",
      "Model: dynamic_range_model.tflite\n",
      "Weight data types: {<class 'numpy.int8'>, <class 'numpy.float32'>, <class 'numpy.int32'>}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters compared to previous conversion\n",
    "! ls -lh Models/mnist_base.tflite Models/dynamic_range_model.tflite\n",
    "\n",
    "print_model_details(\"Models/mnist_base.tflite\")\n",
    "print_model_details(\"Models/dynamic_range_model.tflite\")\n",
    "\n",
    "check_weight_types(\"Models/mnist_base.tflite\")\n",
    "check_weight_types(\"Models/dynamic_range_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f470771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to [0,1] for float32 consistency\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc61b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743172453.576107   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743172453.576135   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 15:34:13.576361: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/mnist\n",
      "2025-03-28 15:34:13.578140: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 15:34:13.578162: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/mnist\n",
      "2025-03-28 15:34:13.585509: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 15:34:13.609302: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/mnist\n",
      "2025-03-28 15:34:13.620294: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 43935 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# code for step 4.2\n",
    "# Perform Full int8 quantization\n",
    "\n",
    "# we need to stimate the range, i.e., (min, max) of all floating-point tensors in the model\n",
    "def representative_dataset():\n",
    "  indices = np.random.choice(len(train_images), 200, replace=False) # take evenely/randomly distrubted from data\n",
    "  for i in indices:\n",
    "      yield [np.expand_dims(train_images[i], axis=0)]\n",
    "\n",
    "# set up converter\n",
    "int8_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + 'Models/mnist') # path to the SavedModel directory\n",
    "int8_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_converter.representative_dataset = representative_dataset\n",
    "int8_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_quant_model = int8_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db0887ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_model.tflite) Accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance(path_to_lab + 'Models/int8_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bc3b4",
   "metadata": {},
   "source": [
    "**Q1: Do you see any difference in accuracy? What is changed in terms of the model parameters compared to the previous conversion?**\n",
    "    \n",
    "There is no change in accuracy as it stays 0.9912.\n",
    "\n",
    "Simply comparing the size of the files we see a +- 3.5x reduction&nbsp;\n",
    "\n",
    "- 68K Mar 28 13:40 Models/dynamic_range_model.tflite\n",
    "- 248K Mar 28 13:51 Models/mnist_base.tflite\n",
    "\n",
    "Taking a closer look at the parameters we see there is no difference in amount of tensors between the models but some tensor have different datatypes in the quantized model compared to base, conversion of some tensors from float/int32 to int8 lead to this reduction in size:\n",
    "\n",
    "- Model: mnist_base.tflite \\\n",
    "Weight data types: {<class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}\n",
    "\n",
    "- Model: dynamic_range_model.tflite \\\n",
    "Weight data types: {<class 'numpy.int8'=\"\">, <class 'numpy.float32'=\"\">, <class 'numpy.int32'=\"\">}</class></class></class></class></class>\n",
    "    \n",
    "**Q2: Compared to dynamic range quantization, what accuracy difference do you get with full int8 precision quantization?**\n",
    "There is a slight drop (accuracy = 0.9877) if we use only 100 images to represent the data.\n",
    "\n",
    "If we use 200 images we can narrow the gap (accuracy = 0.9905).\n",
    "\n",
    "And there is an even better result if we don't naively select the first 200 images. Instead I used `np.linspace` to try and represent the whole dataset better. This resulted in a slight performance boost (accuracy = 0.9910)\n",
    "\n",
    "With random selection we can get even better results (accuracy = 0.9914), this random approach probably only works good if we use a big enough subset of the data.\n",
    "\n",
    "This makes sense since we have to try and represent the range the value's in our dataset can take so the values of S and Z get estimated with more realistic r_min and r_max."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5a233",
   "metadata": {},
   "source": [
    "5) Try to train the model from scratch using **quantization-aware training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c25a7",
   "metadata": {},
   "source": [
    "Full training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f33554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "937/938 [============================>.] - ETA: 0s - loss: 1.9020 - accuracy: 0.3141\n",
      "Epoch 1: val_loss improved from inf to 1.86321, saving model to Models/best_fits/q_aware_model\n",
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/best_fits/q_aware_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 19s 19ms/step - loss: 1.9020 - accuracy: 0.3141 - val_loss: 1.8632 - val_accuracy: 0.2813\n",
      "Epoch 2/50\n",
      "929/938 [============================>.] - ETA: 0s - loss: 1.2567 - accuracy: 0.5430"
     ]
    }
   ],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training\n",
    "model = mnist_model(train=False)\n",
    "model = tf.keras.models.clone_model(model)  # we need weights from scratch\n",
    "\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# keep best fit\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path_to_lab + \"Models/best_fits/q_aware_model\", \n",
    "                    monitor=\"val_loss\", mode=\"min\", \n",
    "                    save_best_only=True, verbose=1)\n",
    "\n",
    "# train from scratch\n",
    "q_aware_model.fit(x=train_images, y= train_labels, batch_size=64, epochs=50, validation_data=(test_images, test_labels), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ed529",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model.load_weights(path_to_lab + \"Models/best_fits/q_aware_model\")\n",
    "q_aware_model.save(path_to_lab + \"Models/qat_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4066895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0339 - accuracy: 0.9899 - 838ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db87a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743174169.160433   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743174169.160454   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 16:02:49.160644: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/qat_mnist\n",
      "2025-03-28 16:02:49.164511: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 16:02:49.164540: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/qat_mnist\n",
      "2025-03-28 16:02:49.180340: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 16:02:49.245189: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/qat_mnist\n",
      "2025-03-28 16:02:49.266143: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 105503 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_mnist\") # path to the SavedModel directory\n",
    "int8_qat_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_converter.representative_dataset = representative_dataset\n",
    "int8_qat_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_model = int8_qat_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d381b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_model.tflite) Accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "# Verify performance\n",
    "verify_peformance('Models/int8_qat_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e798b",
   "metadata": {},
   "source": [
    "Fine tuning model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f93f6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 17s 17ms/step - loss: 0.2389 - accuracy: 0.9377 - val_loss: 0.0894 - val_accuracy: 0.9861\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0833 - accuracy: 0.9857 - val_loss: 0.0561 - val_accuracy: 0.9901\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0517 - accuracy: 0.9894 - val_loss: 0.0438 - val_accuracy: 0.9898\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0427 - accuracy: 0.9896 - val_loss: 0.0408 - val_accuracy: 0.9909\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0442 - accuracy: 0.9890 - val_loss: 0.0442 - val_accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0450 - accuracy: 0.9884 - val_loss: 0.0375 - val_accuracy: 0.9903\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0420 - accuracy: 0.9890 - val_loss: 0.0437 - val_accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0421 - accuracy: 0.9893 - val_loss: 0.0361 - val_accuracy: 0.9918\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 19s 21ms/step - loss: 0.0431 - accuracy: 0.9893 - val_loss: 0.0380 - val_accuracy: 0.9902\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 18s 20ms/step - loss: 0.0460 - accuracy: 0.9886 - val_loss: 0.0384 - val_accuracy: 0.9903\n",
      "INFO:tensorflow:Assets written to: Models/qat_fine_time_mnist/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/qat_fine_time_mnist/assets\n"
     ]
    }
   ],
   "source": [
    "# code for step 5\n",
    "# Perform Quantization aware training\n",
    "model = mnist_model(train=False)\n",
    "\n",
    "q_aware_model_fine_tune = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "q_aware_model_fine_tune.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "# train from scratch\n",
    "q_aware_model_fine_tune.fit(x=train_images, y= train_labels, batch_size=64, epochs=10, validation_data=(test_images, test_labels))\n",
    "q_aware_model_fine_tune.save(path_to_lab + \"Models/qat_fine_time_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bbd4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0384 - accuracy: 0.9903 - 915ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = q_aware_model_fine_tune.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7c3eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743172973.088585   27551 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743172973.088606   27551 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-28 15:42:53.088859: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: Models/qat_fine_time_mnist\n",
      "2025-03-28 15:42:53.092933: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-28 15:42:53.092954: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: Models/qat_fine_time_mnist\n",
      "2025-03-28 15:42:53.113269: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-28 15:42:53.200226: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: Models/qat_fine_time_mnist\n",
      "2025-03-28 15:42:53.228580: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 139728 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "# set up converter\n",
    "int8_qat_ft_converter = tf.lite.TFLiteConverter.from_saved_model(path_to_lab + \"Models/qat_fine_time_mnist\") # path to the SavedModel directory\n",
    "int8_qat_ft_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "int8_qat_ft_converter.representative_dataset = representative_dataset\n",
    "int8_qat_ft_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # only int8\n",
    "\n",
    "# acutally convert and save\n",
    "tflite_int8_qat_ft_model = int8_qat_ft_converter.convert() \n",
    "with open(path_to_lab + 'Models/int8_qat_ft_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_int8_qat_ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989c8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model (int8_qat_ft_model.tflite) Accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "verify_peformance('Models/int8_qat_ft_model.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df753a",
   "metadata": {},
   "source": [
    "**Q3: What is the impact on accuracy with quantization-aware training?**\n",
    "\n",
    "If we train a mnist model from scratch (clone before applying callingquantize_model(model)) with 50 epochs we get accuracy = 0.9898\n",
    "If we train a mnnist model with fine-tuning of 10 epchs we get accuracy = 0.9903\n",
    "Both are lower then the results without quantization-aware training which is not what i expected.\n",
    "\n",
    "**Q4: When saving the tflite model, do you see any difference in the model size (full int8 quantization vs no quantization)?**\n",
    "> amsw4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a27c3e",
   "metadata": {},
   "source": [
    "6) **Prune** the first three layers, at 85% AND perform **full INT8 quantization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708804b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for step 6\n",
    "# Perform pruning + quantization\n",
    "# Verify performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08000fa",
   "metadata": {},
   "source": [
    "\n",
    "**Q5: Describe the observed effect in terms of accuracy and zipped model size when performing both pruning (first three layers, 85%) & full int8 quantization. (Tip: check the zipped tflite file size)**]\n",
    "> answ5"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a02c81482558c89362995c8df197637e5b64523b135283abb74d4c68183e29b"
  },
  "kernelspec": {
   "display_name": "Python 3.11.11 ('embedded-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
